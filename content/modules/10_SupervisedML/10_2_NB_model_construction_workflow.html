
<!DOCTYPE html>


<html lang="en" data-content_root="../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Module 10.2: Naive Bayes and Model Construction &#8212; 2026 Cheminformatics OLCC</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'content/modules/10_SupervisedML/10_2_NB_model_construction_workflow';</script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Appendix 1.1: Getting Set Up" href="../../appendices/App_1/README.html" />
    <link rel="prev" title="10.1: Data Preparation and Feature Engineering" href="10_1_data_prep.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../../_static/logo.png" class="logo__image only-light" alt="2026 Cheminformatics OLCC - Home"/>
    <script>document.write(`<img src="../../../_static/logo.png" class="logo__image only-dark" alt="2026 Cheminformatics OLCC - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    Welcome to your Jupyter Book
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Core Modules</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../01_PythonPrimer/README.html">Module 1 Python Primers</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../01_PythonPrimer/01_1_python-basics.html">1.1 Python Basics</a></li>


<li class="toctree-l2"><a class="reference internal" href="../01_PythonPrimer/01_2_python-intermediate.html">1.2 Intermediate Python</a></li>


</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="README.html">Module 10: Supervised Machine Learning</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="10_0_Intro.html">10.0: Intro</a></li>




<li class="toctree-l2"><a class="reference internal" href="10_1_data_prep.html">10.1: Data Preparation and Feature Engineering</a></li>






<li class="toctree-l2 current active"><a class="current reference internal" href="#">Module 10.2: Naive Bayes and Model Construction</a></li>





</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendices</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../appendices/App_1/README.html">Appendix 1.1: Getting Set Up</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../appendices/App_1/A_01-1_OS.html">Appendix 1: Operating System</a></li>



</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../appendices/App_10/README.html">Appendix 10: Supervised Machine Learning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../appendices/App_10/A_10-1AID_Selector.html">A10.1 BioAssay Screening: Enough Actives</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../appendices/App_10/A_10-2NaiveBayes.html">A10.2 Bayes’ Theorem: From Inference to Models</a></li>



</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://nanohub.org/tools/datachemolcc/hub/user-redirect/git-pull?repo=https%3A//github.com/rebelford/2026Cheminformatics&urlpath=lab/tree/2026Cheminformatics/content/modules/10_SupervisedML/10_2_NB_model_construction_workflow.ipynb&branch=main" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on JupyterHub"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="JupyterHub logo" src="../../../_static/images/logo_jupyterhub.svg">
  </span>
<span class="btn__text-container">JupyterHub</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/rebelford/2026Cheminformatics" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/rebelford/2026Cheminformatics/issues/new?title=Issue%20on%20page%20%2Fcontent/modules/10_SupervisedML/10_2_NB_model_construction_workflow.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../../_sources/content/modules/10_SupervisedML/10_2_NB_model_construction_workflow.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Module 10.2: Naive Bayes and Model Construction</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Module 10.2: Naive Bayes and Model Construction</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#preparation-for-model-building">1. Preparation for model building</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#loading-the-data-into-x-and-y">1.1 Loading the data into X and y.</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-selection-remove-zero-variance-features">1.2 Feature Selection: Remove zero-variance features</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-selection-freezing-the-feature-definition-mask-and-metadata">1.3 Feature Selection: Freezing the Feature Definition (Mask and Metadata)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reload-data">1.4 Reload Data</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regenerate-x-maccs">1.4.1 Regenerate X_MACCS</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#load-and-apply-saved-variance-mask">1.4.2 Load and apply saved variance mask</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reconstruct-the-filtered-feature-matrix-x-maccs-filtered">1.4.3 Reconstruct the filtered feature matrix <code class="docutils literal notranslate"><span class="pre">X_MACCS_filtered</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#train-test-split-a-9-1-ratio">1.5 Train-Test-Split (a 9:1 ratio)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#save-the-test-train-split-as-np-arrays">1.5.1 Save the test/train split as np arrays</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#preparing-the-training-data-model-construction">2. Preparing the Training Data Model Construction</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reload-the-saved-training-and-test-arrays">2.1 Reload the Saved Training and Test Arrays</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#examine-class-imbalance">2.2 Examine class imbalance</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-class-imbalance-and-why-it-matters">2.2.1 What is class imbalance and why it matters</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#measuring-class-imbalance-in-the-training-set">2.2.2 Measuring Class Imbalance in the Training Set</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#balance-the-training-set-by-downsampling">2.3 Balance the Training Set by Downsampling</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">2.3 Balance the training set by downsampling</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#build-a-model-using-the-training-set">3. Build a model using the training set.</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#naive-bayes">3.1 Naïve Bayes</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#from-training-to-inference-what-does-the-classifier-produce">3.2 From Training to Inference: What does the Classifier Produce?</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#classification-based-inference-predict">4. Classification-Based Inference (<code class="docutils literal notranslate"><span class="pre">.predict</span></code>)</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#confusion-matrix-for-classification-evaluation">4.1 Confusion Matrix for Classification Evaluation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#load-feature-arrays-and-generate-the-confusion-matrix">4.2 Load Feature Arrays and Generate the Confusion Matrix</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#saving-confusion-matrices-and-metrics">4.3 Saving Confusion Matrices and Metrics</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recording-experimental-metadata">4.3.1 Recording Experimental Metadata</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interpreting-the-confusion-matrix">4.4 Interpreting the Confusion Matrix</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#confusion-matrix-based-evaluation-metrics">4.5 Confusion Matrix Based Evaluation Metrics</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#table-of-confusion-matrix-metrics">4.5.1 Table of Confusion Matrix Metrics</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-based-inference-predict-proba">5. Probability-Based Inference (<code class="docutils literal notranslate"><span class="pre">.predict_proba</span></code>)</a></li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="module-10-2-naive-bayes-and-model-construction">
<h1>Module 10.2: Naive Bayes and Model Construction<a class="headerlink" href="#module-10-2-naive-bayes-and-model-construction" title="Link to this heading">#</a></h1>
<div class="alert alert-block alert-warning">
<h1>Learning Objectives</h1>
<p><strong>Purpose</strong><br />
Introduce students to the manual construction of a supervised classification
model using Naïve Bayes, emphasizing how data preparation, feature selection,
and training decisions affect model behavior and evaluation.</p>
<p><strong>Students Learn</strong></p>
<ul>
  <li>Define a fixed feature space by removing invariant fingerprint bits and preserving the feature mask.</li>
  <li>Create, save, and reload stratified training and test splits along with supporting metadata.</li>
  <li>Diagnose class imbalance in a training dataset and apply downsampling as a model-specific preprocessing step.</li>
  <li>Build a probabilistic Naïve Bayes classifier from training data.</li>
  <li>Generate and interpret confusion matrices for classification-based inference.</li>
  <li>Generate and interpret ROC curves for probability-based model evaluation.</li>
</ul>
<p><strong>Core Activities</strong></p>
<ul>
  <li>Organize model inputs, splits, and metadata into a reproducible directory structure.</li>
  <li>Construct a Naïve Bayes classifier step-by-step using saved training data.</li>
  <li>Evaluate model performance using both class predictions and predicted probabilities.</li>
</ul>
<p><strong>Prior Knowledge</strong></p>
<ul>
  <li>Complete Module 10.1: <a href="content/modules/10_SupervisedML/10_1_data_prep.html">Data Preparation and Feature Engineering</a></li>
  <li>Complete Appendix A10.2: <a href="https://rebelford.github.io/2026Cheminformatics/content/appendices/App_10/A_10-2NaiveBayes.html">Bayes' Theorem: From Inference to Models</a></li>
</ul>
</div>
<div class="alert alert-block alert-warning">
<h1>Learning Objectives</h1>
<p><strong>Purpose</strong>  Introduce students to the manual construction of a model using Naive Bayes.</p>
<p><strong>Students Learn</strong></p>
<ul class="simple">
<li><p>Remove invariant features and save Test/Training Splits</p></li>
<li><p>Store Test/Training splits and metadata</p></li>
<li><p>Balance an imbalanced set through downsizing</p></li>
<li><p>Build a Naïve Bayes classifier using training data.</p></li>
<li><p>Generate a Confusion Matrix</p></li>
<li><p>Generate an ROC curve</p></li>
</ul>
<p><strong>Core Activities</strong></p>
<ul class="simple">
<li><p>Create a directory with data artifacts</p></li>
<li></li>
</ul>
<p><strong>Prior Knowledge</strong></p>
<ul class="simple">
<li><p>Complete Module 10.2 [Data Preparation and Feature Engineering](file://wsl.localhost/Ubuntu/home/rebelford/jupyterbooks/cinf26book/_build/html/content/modules/10_SupervisedML/10_1_data_prep.html)</p></li>
<li><p>Complete Appendix A10.2: <a class="reference external" href="https://rebelford.github.io/2026Cheminformatics/content/appendices/App_10/A_10-2NaiveBayes.html">Bayes’ Theorem: From Inference to Models</a></p></li>
</ul>
<p><strong>Reading links</strong></p>
<ul class="simple">
<li><p><a class="reference external" href="https://rebelford.github.io/py4scibook/content/module3-packages1/03-2-pandas.html">Pandas Chapter of Py4Sci</a></p></li>
</ul>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="preparation-for-model-building">
<h1>1. Preparation for model building<a class="headerlink" href="#preparation-for-model-building" title="Link to this heading">#</a></h1>
<p>We are building supervised learning models to predict the biological activity of small molecules. Each molecule is represented by a set of MACCS keys, and each molecule has an associated activity label: <strong>1 for active</strong> and <strong>0 for inactive</strong>.</p>
<p>For a <em>single compound</em>, the model can be written as:
$<span class="math notranslate nohighlight">\(y = f(X)\)</span>$
where:</p>
<ul class="simple">
<li><p>(X) is a <strong>feature vector</strong> containing the MACCS keys for that compound,</p></li>
<li><p>(y) is a <strong>scalar</strong> prediction (0 or 1),</p></li>
<li><p>(f) is the learned model that maps features to an activity prediction.</p></li>
</ul>
<p>For an <em>entire dataset</em> of compounds, this generalizes to:
$<span class="math notranslate nohighlight">\(
\mathbf{y} = f(\mathbf{X})
\)</span>$
where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\((\mathbf{X})\)</span> is a <strong>feature matrix</strong> of shape <em>(n_samples, n_features)</em>,</p></li>
<li><p><span class="math notranslate nohighlight">\((\mathbf{y})\)</span> is a <strong>label vector</strong> of length <em>n_samples</em>.</p></li>
</ul>
<p>We use uppercase X to indicate that the input data is multiple descriptors</p>
<p>In practice, the symbol X is used generically to denote the input features. Depending on context, X may refer to:</p>
<ul class="simple">
<li><p>a single feature vector for one compound.</p></li>
<li><p>A feature matrix containing feature vectors for many compounds</p></li>
</ul>
<div class="alert alert-block alert-info">
<strong>Deeper Dive:</strong> Let’s take a closer look at <em>y = f(X)</em>
  <div style="
    background-color: #efffff;
    color: #000000;
    padding: 10px;
    border-radius: 4px;
    border: 1px solid #dddddd;
    margin-top: 10px;
  ">
<details>
<summary>Explanation</summary>
<p>
The use of uppercase <b>X</b> and lowercase <b>y</b> follows a long-standing
convention in mathematics, statistics, and machine learning.
</p>
<ul>
  <li>
    <b><code>X</code> (uppercase)</b> typically represents a <b>matrix of features</b> when working with a dataset
    <ul>
      <li>Shape: <i>(n_samples, n_features)</i></li>
      <li>Each row corresponds to one compound (observation)</li>
      <li>Each column corresponds to one descriptor or fingerprint bit</li>
    </ul>
  </li>
  <br>
  <li>
    <b><code>y</code> (lowercase)</b> represents a <b>vector of target values (labels)</b>
    <ul>
      <li>Shape: <i>(n_samples,)</i></li>
      <li>Each element is the activity associated with one compound</li>
    </ul>
  </li>
</ul>
<p>In linear-algebra terms:</p>
<ul>
  <li><code>X</code> is a 2-dimensional object (a matrix)</li>
  <li><code>y</code> is a 1-dimensional object (a vector)</li>
</ul>
<p>
This naming convention reflects the canonical supervised-learning equation:
</p>
<p style="text-align: center;">
\( y = f(\mathbf{X}) \)
</p>
<p>where:</p>
<ul>
  <li>the function <code>f</code> represents a learned model</li>
  <li>the model takes a matrix of input features (<b>X</b>)</li>
  <li>and produces a vector of outputs (<b>y</b>)</li>
</ul>
<p><b>In short:</b></p>
<ul>
  <li><b>Uppercase <code>X</code></b> → feature matrix</li>
  <li><b>Lowercase <code>y</code></b> → target vector</li>
</ul>
<p>
This convention is not enforced by Python, but it is widely adopted and helps
communicate the structure of the data at a glance.
</p>
</details>
</div>
</div>
<section id="loading-the-data-into-x-and-y">
<h2>1.1 Loading the data into X and y.<a class="headerlink" href="#loading-the-data-into-x-and-y" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="c1"># read fingerprints with activities csv file into pandas dataframe</span>
<span class="n">df_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;data/AID743139/features/AID743139_MACCS_activites_noSalt_20260118_v1.csv&quot;</span><span class="p">)</span> <span class="c1"># Change to your file path if you need to load the dataframe</span>

<span class="c1"># we now have a dataframe with CIDS, activities and maccs keys</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df_data</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">df_data</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(6793, 170)
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>cid</th>
      <th>activity</th>
      <th>clean_smiles</th>
      <th>maccs000</th>
      <th>maccs001</th>
      <th>maccs002</th>
      <th>maccs003</th>
      <th>maccs004</th>
      <th>maccs005</th>
      <th>maccs006</th>
      <th>...</th>
      <th>maccs157</th>
      <th>maccs158</th>
      <th>maccs159</th>
      <th>maccs160</th>
      <th>maccs161</th>
      <th>maccs162</th>
      <th>maccs163</th>
      <th>maccs164</th>
      <th>maccs165</th>
      <th>maccs166</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>12850184</td>
      <td>0</td>
      <td>O=C(CO)[C@@H](O)[C@H](O)[C@@H](O)C(=O)[O-]</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>89753</td>
      <td>0</td>
      <td>O=C([O-])[C@H](O)[C@@H](O)[C@H](O)[C@H](O)CO</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>9403</td>
      <td>0</td>
      <td>C[C@]12CC[C@@H]3c4ccc(O)cc4CC[C@H]3[C@@H]1CC[C...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>3 rows × 170 columns</p>
</div></div></div>
</div>
<p>We will put the MACCS Keys into a variable called X_MACCS, the feature Matrix.<br>
We will put the activity values into a variable called y, the label vector.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_MACCS</span> <span class="o">=</span> <span class="n">df_data</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">3</span><span class="p">:]</span> <span class="c1"># this is dropping cid, activity, and clean_smiles and creating a new variable for maccs data</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df_data</span><span class="p">[</span><span class="s1">&#39;activity&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span> <span class="c1"># note, df_data is a pd dataframe, but .values makes it a np array</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Print head of feature matrix</span>
<span class="n">X_MACCS</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>maccs000</th>
      <th>maccs001</th>
      <th>maccs002</th>
      <th>maccs003</th>
      <th>maccs004</th>
      <th>maccs005</th>
      <th>maccs006</th>
      <th>maccs007</th>
      <th>maccs008</th>
      <th>maccs009</th>
      <th>...</th>
      <th>maccs157</th>
      <th>maccs158</th>
      <th>maccs159</th>
      <th>maccs160</th>
      <th>maccs161</th>
      <th>maccs162</th>
      <th>maccs163</th>
      <th>maccs164</th>
      <th>maccs165</th>
      <th>maccs166</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>3 rows × 167 columns</p>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span><span class="c1"># prints first 5 values of the label vector </span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">))</span> <span class="c1"># prints all unique values of label vector</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0 0 0 0 0]
[0 1]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Write the code to print length of label vector (y) and number of active compounds (hint, they are zeros and ones, so you can sum them)</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>6793
743
</pre></div>
</div>
</div>
</div>
</section>
<section id="feature-selection-remove-zero-variance-features">
<h2>1.2 Feature Selection: Remove zero-variance features<a class="headerlink" href="#feature-selection-remove-zero-variance-features" title="Link to this heading">#</a></h2>
<p>Some features in X are not helpful in distinguishing actives from inactives, because they are set ON for all compounds or OFF for all compounds.  Such features need to be removed because they would consume more computational resources without improving the model.</p>
<p>We will use the <code class="docutils literal notranslate"><span class="pre">VarianceThreshold</span></code> method of sklearn to identify which features have a variance of zero or very low. Variance in data represents how spread out the values of a feature are. The <code class="docutils literal notranslate"><span class="pre">threshold</span></code> parameter is set to 0.0 by default, meaning only features with zero variance (constant values across all samples, 100% identical values) are removed.</p>
<div class="alert alert-block alert-info">
<p><strong>What if a feature has ≥99% identical values?</strong></p>
   <div style="
    background-color: #efffff;
    color: #000000;
    padding: 10px;
    border-radius: 4px;
    border: 1px solid #dddddd;
    margin-top: 10px;
  ">       
<details>
<summary>Explanation</summary>
Let’s say a feature is <code>1</code>  in 99.5% of rows and <code>0</code>  in the remaining 0.5%. It does not have zero variance, but the variance is very low.
<p>If you want to remove such near-constant features, you need to set <code>threshold</code> accordingly. In this case the variance is calculated as:</p>
<center>Var(X)=<i>p</i>(1−<i>p</i>)=0.995×(1−0.995)=0.004975<br>
where <i>p</i> is probability of the feature being 1</center>
<p>So to remove this feature, your threshold must be greater than 0.004975, for example:
<code>VarianceThreshold(threshold=0.005)</code></p>
<p>It might be interesting to see how our models change, or time calculating the model changes if we do some prefiltering by adjusting the threshold.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.feature_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">VarianceThreshold</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_MACCS</span><span class="o">.</span><span class="n">shape</span>  <span class="c1">#- Before removal</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(6793, 167)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.feature_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">VarianceThreshold</span>

<span class="c1"># Apply variance threshold</span>
<span class="n">sel</span> <span class="o">=</span> <span class="n">VarianceThreshold</span><span class="p">(</span><span class="n">threshold</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
<span class="n">X_MACCS_filtered</span> <span class="o">=</span> <span class="n">sel</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_MACCS</span><span class="p">)</span>

<span class="c1"># Boolean mask of retained features</span>
<span class="n">mask</span> <span class="o">=</span> <span class="n">sel</span><span class="o">.</span><span class="n">get_support</span><span class="p">()</span>

<span class="c1"># Human-readable feature names</span>
<span class="n">kept_features</span> <span class="o">=</span> <span class="n">X_MACCS</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
<span class="n">removed_features</span> <span class="o">=</span> <span class="n">X_MACCS</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="o">~</span><span class="n">mask</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Features removed:&quot;</span><span class="p">,</span> <span class="nb">list</span><span class="p">(</span><span class="n">removed_features</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Filtered feature matrix shape:&quot;</span><span class="p">,</span> <span class="n">X_MACCS_filtered</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># ---- FEATURE METADATA (generated here) ----</span>
<span class="n">feature_metadata</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;fingerprint&quot;</span><span class="p">:</span> <span class="s2">&quot;MACCS&quot;</span><span class="p">,</span>
    <span class="s2">&quot;original_bits&quot;</span><span class="p">:</span> <span class="n">X_MACCS</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
    <span class="s2">&quot;selected_bits&quot;</span><span class="p">:</span> <span class="nb">int</span><span class="p">(</span><span class="n">mask</span><span class="o">.</span><span class="n">sum</span><span class="p">()),</span>
    <span class="s2">&quot;removed_bits&quot;</span><span class="p">:</span> <span class="nb">int</span><span class="p">((</span><span class="o">~</span><span class="n">mask</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()),</span>
    <span class="s2">&quot;selection_method&quot;</span><span class="p">:</span> <span class="s2">&quot;VarianceThreshold&quot;</span><span class="p">,</span>
    <span class="s2">&quot;threshold&quot;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="s2">&quot;removed_feature_names&quot;</span><span class="p">:</span> <span class="nb">list</span><span class="p">(</span><span class="n">removed_features</span><span class="p">),</span>
    <span class="s2">&quot;source_file&quot;</span><span class="p">:</span> <span class="s2">&quot;AID743139_MACCS_activites_noSalt_20260104_v1.csv&quot;</span><span class="p">,</span>
    <span class="s2">&quot;notes&quot;</span><span class="p">:</span> <span class="s2">&quot;Invariant MACCS bits removed prior to modeling&quot;</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Features removed: [&#39;maccs000&#39;, &#39;maccs001&#39;, &#39;maccs002&#39;, &#39;maccs004&#39;, &#39;maccs166&#39;]
Filtered feature matrix shape: (6793, 162)
</pre></div>
</div>
</div>
</div>
<div class="alert alert-block alert-info">
<strong>Check your understanding</strong>
<p>A MACCS fingerprint has 166 bit positions but the RDKit MACCS fingerprint has 167. This position has zeros for all molecules and is removed regardless of the threshold, can you think of why this position is set to zero?</p>
  <div style="
    background-color: #efffff;
    color: #000000;
    padding: 10px;
    border-radius: 4px;
    border: 1px solid #dddddd;
    margin-top: 10px;
  ">
<details>
<summary>Answer</summary>
The <code>maccs000</code> position is always zero because it is a
<strong>dummy bit</strong> added by RDKit for bookkeeping purposes.
<p>The canonical MACCS fingerprint defines 166 chemically meaningful
keys numbered 1–166 (not 167).</p>
<p>By including a zero-valued bit at position 0, RDKit allows the bit index
to match the MACCS key number directly (e.g., bit 1 → MACCS key 1),
avoiding off-by-one confusion when labeling features.</p> Because this dummy bit carries no chemical information and has zero
variance across all molecules, it is removed during feature preprocessing.
</details>
</div>
</div>
</section>
<section id="feature-selection-freezing-the-feature-definition-mask-and-metadata">
<h2>1.3 Feature Selection: Freezing the Feature Definition (Mask and Metadata)<a class="headerlink" href="#feature-selection-freezing-the-feature-definition-mask-and-metadata" title="Link to this heading">#</a></h2>
<p>Up to this point, we have generated MACCS fingerprints for all compounds and examined which fingerprint bits vary across the dataset. When we remove invariant bits using a variance filter, we are no longer just manipulating data—we are defining the feature space that every downstream model will use. From this moment forward, a “feature” has a specific meaning: it is one of the MACCS bits that survived this filtering step. This process is known as feature selection: deciding which descriptors are informative enough to be included in the model and which should be excluded.</p>
<p>Because this feature selection step is learned from the data, it must be treated as part of the scientific record. Any model trained on these data—whether Naive Bayes, Decision Trees, or future methods—must use exactly the same feature definition in order to be valid and comparable. To ensure reproducibility and avoid hidden assumptions, we explicitly save the feature mask (a Boolean array indicating which MACCS bits were kept or removed) as a numpy file, along with descriptive metadata explaining how and why the selection was performed as a json file.</p>
<p>By freezing the feature definition here (meaning it will not change for this dataset), we create a clear boundary between data preparation and modeling. All subsequent notebooks and models will load and reuse this saved feature definition rather than recomputing it, guaranteeing that results remain consistent even across kernel restarts, new environments, or alternative machine-learning methods.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">pathlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">Path</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">json</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pchelpfct</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pchf</span>

<span class="n">PROJECT_ROOT</span> <span class="o">=</span> <span class="n">Path</span><span class="o">.</span><span class="n">cwd</span><span class="p">()</span>  <span class="c1"># current working directory</span>
<span class="n">FEATURES</span> <span class="o">=</span> <span class="n">PROJECT_ROOT</span> <span class="o">/</span> <span class="s2">&quot;data&quot;</span> <span class="o">/</span> <span class="s2">&quot;AID743139&quot;</span> <span class="o">/</span> <span class="s2">&quot;features&quot;</span>
<span class="n">FEATURES</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">parents</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Save variance mask</span>
<span class="n">mask_fname</span> <span class="o">=</span> <span class="n">pchf</span><span class="o">.</span><span class="n">make_filename</span><span class="p">(</span>
    <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;AID743139_MACCS_variance_mask&quot;</span><span class="p">,</span>
    <span class="n">ext</span><span class="o">=</span><span class="s2">&quot;npy&quot;</span><span class="p">,</span>
    <span class="n">folder</span><span class="o">=</span><span class="nb">str</span><span class="p">(</span><span class="n">FEATURES</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">FEATURES</span> <span class="o">/</span> <span class="n">mask_fname</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>

<span class="c1"># Save metadata JSON</span>
<span class="n">meta_fname</span> <span class="o">=</span> <span class="n">pchf</span><span class="o">.</span><span class="n">make_filename</span><span class="p">(</span>
    <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;AID743139_MACCS_feature_metadata&quot;</span><span class="p">,</span>
    <span class="n">ext</span><span class="o">=</span><span class="s2">&quot;json&quot;</span><span class="p">,</span>
    <span class="n">folder</span><span class="o">=</span><span class="nb">str</span><span class="p">(</span><span class="n">FEATURES</span><span class="p">)</span>
<span class="p">)</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">FEATURES</span> <span class="o">/</span> <span class="n">meta_fname</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">json</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">feature_metadata</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[Saved] Variance mask → </span><span class="si">{</span><span class="n">mask_fname</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[Saved] Feature metadata → </span><span class="si">{</span><span class="n">meta_fname</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[Saved] Variance mask → /home/rebelford/jupyterbooks/cinf26book/content/modules/10_SupervisedML/data/AID743139/features/AID743139_MACCS_variance_mask_20260128_v1.npy
[Saved] Feature metadata → /home/rebelford/jupyterbooks/cinf26book/content/modules/10_SupervisedML/data/AID743139/features/AID743139_MACCS_feature_metadata_20260128_v1.json
</pre></div>
</div>
</div>
</div>
<div class="alert alert-block alert-success">
<strong>Code Dive</strong>
<p>Explain the role of the following three files in the features directory. These three files together define the feature representation and labeling scheme used for model training and evaluation. They must be treated as a matched set.
</p>
  <div style="
    background-color: #efffff;
    color: #000000;
    padding: 10px;
    border-radius: 4px;
    border: 1px solid #dddddd;
    margin-top: 10px;
  ">
<details>
    <summary>AID743139_MACCS_activites_noSalt_.csv</summary>
This CSV file contains the core machine-learning dataset. Each row corresponds to a single PubChem compound (CID) and includes:
<ul>
    <li>The PubChem Compound ID (CID)</li>
    <li>A binary activity label (0 = inactive, 1 = active) derived from the assay outcomes</li>
    <li>A MACCS fingerprint vector, which will be used to construct the feature matrix (X) for model training</li>
</ul>
The MACCS fingerprints were generated from salt-stripped molecules, keeping only the largest molecular fragment so that counterions and formulation artifacts do not influence the feature representation. This file is the direct input to scikit-learn for building and evaluating classification models.
</details>
</div>
  <div style="
    background-color: #efffff;
    color: #000000;
    padding: 10px;
    border-radius: 4px;
    border: 1px solid #dddddd;
    margin-top: 10px;
  ">
<details>
    <summary>AID743139_MACCS_feature_metadata_.json</summary>
This JSON file records the feature-engineering decisions applied to the MACCS fingerprints. It serves as documentation and provenance for how the feature matrix was constructed.
<p>Typical contents include:</p>
<ul>
    <li>Which MACCS bit positions were removed (e.g., invariant bits such as MACCS000)</li>
    <li>The reason for removal (e.g., zero variance across all compounds)</li>
    <li>The original fingerprint length and the final feature count</li>
    <li>Any parameters or assumptions used during fingerprint processing</li>
</ul>
This file allows us to:
<ul>
    <li>Reproduce the feature matrix exactly</li>
    <li>Understand why certain bits are missing</li>
    <li>Apply the same preprocessing rules to future datasets or external test compounds</li>
</ul>
</details>
</div>
  <div style="
    background-color: #efffff;
    color: #000000;
    padding: 10px;
    border-radius: 4px;
    border: 1px solid #dddddd;
    margin-top: 10px;
  ">
<details>
    <summary>AID743139_MACCS_variance_mask_.npy</summary>
<p>
This NumPy file contains the Boolean variance mask used to filter the MACCS fingerprints. The mask is a 1-D array with one entry per original MACCS bit:
<ul>
    <li>True → keep this bit</li>
    <li>False → drop this bit</li>
</ul>
During feature construction, this mask is applied to every fingerprint so that only informative (non-invariant) features are retained. The mask is the operational object that enforces the rules described in the metadata JSON.
This file is used programmatically to:
<ul>
    <li>Transform raw MACCS fingerprints into the final feature matrix</li>
    <li>Ensure consistency between training data, test data, and future predictions</li>
</ul>
</p>
</details>
</div>
</div>
</section>
<section id="reload-data">
<h2>1.4 Reload Data<a class="headerlink" href="#reload-data" title="Link to this heading">#</a></h2>
<p><strong>Where we are in the Supervised Learning Pipeline</strong>
In this module, we are continuing a supervised learning pipeline that began with data preparation. We first downloaded BioAssay data from PubChem and stored the unmodified exports in the <code class="docutils literal notranslate"><span class="pre">/raw</span></code> directory. We then curated these data to remove invalid records and standardize chemical representations, saving the cleaned results in <code class="docutils literal notranslate"><span class="pre">/curated</span></code>. From this curated dataset, we generated molecular fingerprints (MACCS keys) and applied feature-level filtering to remove invariant bits, producing a refined feature representation. The resulting fingerprint dataset, along with the feature-selection mask and metadata documenting how the features were constructed, was saved in the <code class="docutils literal notranslate"><span class="pre">/features</span></code> directory as non-volatile <code class="docutils literal notranslate"><span class="pre">.csv</span></code> and <code class="docutils literal notranslate"><span class="pre">.npy</span></code> files. In the current notebook, we reload these saved feature artifacts and use them to define train/test splits, which establish the experimental framework for the modeling activities that follow. These splits will support multiple supervised learning models—such as Naive Bayes, Decision Trees, Random Forests, and k-Nearest Neighbors—while ensuring that all models are trained and evaluated on a consistent molecular representation.</p>
<p>Note: In this notebook, the term pipeline refers to a conceptual workflow, a sequence of data transformations and modeling steps applied consistently. In later notebooks, this workflow will be formalized into reusable and scalable pipeline objects.</p>
<section id="regenerate-x-maccs">
<h3>1.4.1 Regenerate X_MACCS<a class="headerlink" href="#regenerate-x-maccs" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>X_MACCS is the original unfiltered feature matrix</p></li>
<li><p>This is the “ground truth representation”</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Step 1 Load source feature data</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>

<span class="n">df_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span>
    <span class="s2">&quot;data/AID743139/features/AID743139_MACCS_activites_noSalt_20260118_v1.csv&quot;</span>
<span class="p">)</span><span class="c1"># adjust code to your file</span>

<span class="c1"># Separate labels</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df_data</span><span class="p">[</span><span class="s2">&quot;activity&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>

<span class="c1"># Reconstruct full MACCS feature matrix</span>
<span class="n">X_MACCS</span> <span class="o">=</span> <span class="n">df_data</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">3</span><span class="p">:]</span>  <span class="c1"># CID, activity, clean_smiles dropped</span>

<span class="nb">print</span><span class="p">(</span><span class="n">X_MACCS</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(6793, 167)
</pre></div>
</div>
</div>
</div>
</section>
<section id="load-and-apply-saved-variance-mask">
<h3>1.4.2 Load and apply saved variance mask<a class="headerlink" href="#load-and-apply-saved-variance-mask" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Mask is 1D</p></li>
<li><p>length must equal <code class="docutils literal notranslate"><span class="pre">X_MACCS.shape[1]</span></code></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="c1"># Load saved variance mask</span>
<span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span>
    <span class="s2">&quot;data/AID743139/features/AID743139_MACCS_variance_mask_20260125_v1.npy&quot;</span>
<span class="p">)</span><span class="c1"># adjust code to your file path</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Mask shape:&quot;</span><span class="p">,</span> <span class="n">mask</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">mask</span><span class="o">.</span><span class="n">sum</span><span class="p">(),</span> <span class="s2">&quot;features retained&quot;</span><span class="p">)</span>

<span class="c1"># Safety check: ensure mask matches feature matrix</span>
<span class="k">assert</span> <span class="n">X_MACCS</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span>
    <span class="s2">&quot;Incompatible feature mask: &quot;</span>
    <span class="s2">&quot;mask length does not match number of MACCS features. &quot;</span>
    <span class="s2">&quot;Check that the CSV and mask were generated from the same dataset.&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Mask shape: (167,)
162 features retained
</pre></div>
</div>
</div>
</div>
</section>
<section id="reconstruct-the-filtered-feature-matrix-x-maccs-filtered">
<h3>1.4.3 Reconstruct the filtered feature matrix <code class="docutils literal notranslate"><span class="pre">X_MACCS_filtered</span></code><a class="headerlink" href="#reconstruct-the-filtered-feature-matrix-x-maccs-filtered" title="Link to this heading">#</a></h3>
<p>In this step, we reconstruct the filtered MACCS feature matrix by reapplying a previously learned transformation. No feature selection is performed here; instead, we reuse the saved variance mask to ensure that the exact same feature definition is recovered after a kernel restart. The reconstruction is performed using a Pandas DataFrame so that feature alignment and consistency can be verified. Once feature semantics are finalized, the filtered feature matrix is explicitly converted to a NumPy array in preparation for downstream modeling, where only numeric operations are required.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_MACCS_filtered</span> <span class="o">=</span> <span class="n">X_MACCS</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">mask</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="n">X_MACCS_filtered</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># Decide representation for modeling</span>
<span class="n">X_MACCS_filtered</span> <span class="o">=</span> <span class="n">X_MACCS_filtered</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>   <span class="c1"># &lt;-- optional but explicit</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(6793, 162)
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="train-test-split-a-9-1-ratio">
<h2>1.5 Train-Test-Split (a 9:1 ratio)<a class="headerlink" href="#train-test-split-a-9-1-ratio" title="Link to this heading">#</a></h2>
<p>Now that we’ve prepared the dataset, the next step is to divide it into two parts: one for training the model and one for testing it. This is important because we want to evaluate how well the model performs on unseen data, and not just the data it was trained on.</p>
<p>This is typically done by splitting the dataset into two subsets using a specified ratio. Common splits include 80:20 or 70:30, where the larger portion is used for training and the smaller for testing. When the dataset is small or the model requires more examples to learn effectively, a 90:10 split can be helpful.</p>
<p>In the next code section, we will split the data so that 90% goes into the training set and 10% into the test set. The training set is used to build the model, while the test set is used to evaluate how well the model generalizes to new data. Sklearn’s <code class="docutils literal notranslate"><span class="pre">train_test_split</span></code> creates several NumPy arrays at once by applying the same randomized split to aligned data. We include the DataFrame index so we can recover which chemical compounds ended up in the training and test sets.</p>
<p>It is important that this train–test split is performed before any model-building decisions, such as class balancing, downsampling, or
reweighting. These operations will later be applied only to the training data, never to the test data. By splitting the dataset at this stage and saving the resulting arrays, we preserve an untouched test set that represents the original data distribution. This ensures that model evaluation reflects true generalization performance rather than artifacts introduced during training.</p>
<p>The following table summarizes the key parameters used in the upcoming <code class="docutils literal notranslate"><span class="pre">train_test_split()</span></code> call. Review these options before running the code, as they determine how the training and test sets are constructed.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Parameter</strong></p></th>
<th class="head"><p><strong>Value Used</strong></p></th>
<th class="head"><p><strong>Purpose in This Workflow</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">X_MACCS_filtered</span></code></p></td>
<td><p>feature matrix</p></td>
<td><p>The input feature matrix containing MACCS fingerprints after variance filtering.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">y</span></code></p></td>
<td><p>label vector</p></td>
<td><p>Binary activity labels (0 = inactive, 1 = active) aligned row-by-row with <code class="docutils literal notranslate"><span class="pre">X</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">df_data.index</span></code></p></td>
<td><p>row identifiers</p></td>
<td><p>Preserves the original row identity so compounds can be traced after splitting.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">test_size</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">0.1</span></code></p></td>
<td><p>Holds out 10% of the data for testing, leaving 90% for training.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">shuffle</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">True</span></code></p></td>
<td><p>Randomizes the order of samples before splitting to avoid ordering bias.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">random_state</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">3100</span></code></p></td>
<td><p>Fixes the random number generator seed so the split is reproducible.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">stratify</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">y</span></code></p></td>
<td><p>Ensures the training and test sets retain the same active/inactive class ratio as the original dataset.</p></td>
</tr>
</tbody>
</table>
</div>
<p>In particular, the <code class="docutils literal notranslate"><span class="pre">stratify=y</span></code> argument is critical for classification problems with class imbalance. It guarantees that both the training and test sets reflect the original class distribution, preventing accidental bias in model evaluation caused by uneven splits.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">idx_train</span><span class="p">,</span> <span class="n">idx_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X_MACCS_filtered</span><span class="p">,</span>
    <span class="n">y</span><span class="p">,</span>
    <span class="n">df_data</span><span class="o">.</span><span class="n">index</span><span class="p">,</span>     <span class="c1"># preserve row identity across the split</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">3100</span><span class="p">,</span> <span class="c1"># make the split reproducible</span>
    <span class="n">stratify</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>        <span class="c1"># preserve class balance</span>
    <span class="n">test_size</span><span class="o">=</span><span class="mf">0.1</span>      <span class="c1"># 10% of data held out for testing</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training set shape:&quot;</span><span class="p">,</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;where there are&quot;</span><span class="p">,</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;samples, and&quot;</span><span class="p">,</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;features&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;and&quot;</span><span class="p">,</span> <span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;activities associated with the training set.&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test set shape:&quot;</span><span class="p">,</span> <span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y_test</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;where there are&quot;</span><span class="p">,</span> <span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;samples, and&quot;</span><span class="p">,</span> <span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;features&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;and&quot;</span><span class="p">,</span> <span class="n">y_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;activities associated with the test set.&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of active compounds in training set:&quot;</span><span class="p">,</span> <span class="n">y_train</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of active compounds in test set:&quot;</span><span class="p">,</span> <span class="n">y_test</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training set shape: (6113, 162) (6113,)
where there are 6113 samples, and 162 features
and 6113 activities associated with the training set.

Test set shape: (680, 162) (680,)
where there are 680 samples, and 162 features
and 680 activities associated with the test set.

Number of active compounds in training set: 669
Number of active compounds in test set: 74
</pre></div>
</div>
</div>
</div>
<div class="alert alert-block alert-success"> 
<strong>Code Check:</strong> <p>What does this line of code do?</p>  
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">idx_train</span><span class="p">,</span> <span class="n">idx_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X_MACCS</span><span class="p">,</span>
    <span class="n">y</span><span class="p">,</span>
    <span class="n">df_data</span><span class="o">.</span><span class="n">index</span><span class="p">,</span>     <span class="c1"># preserve row identity across the split</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">3100</span><span class="p">,</span> <span class="c1"># make the split reproducible</span>
    <span class="n">stratify</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>        <span class="c1"># preserve class balance</span>
    <span class="n">test_size</span><span class="o">=</span><span class="mf">0.1</span>      <span class="c1"># 10% of data held out for testing</span>
<span class="p">))</span>
</pre></div>
</div>
<br>
<div class="alert alert-block alert-info">
<details>
    <summary>Explanation</summary>
<p>This line calls scikit-learn’s train_test_split() function to partition the data into training and test sets in a way that is safe for machine learning.</p>
<p>Before this line:</p>
<ul class="simple">
<li><p>X_MACCS_filtered is a 2-D numerical array (or array-like object) containing molecular features
(rows = compounds, columns = MACCS fingerprint bits).</p></li>
<li><p>y is a 1-D numerical array containing the corresponding activity labels
(one value per compound).</p></li>
</ul>
<p><strong>What train_test_split() does:</strong></p>
<ol class="arabic simple">
<li><p>Splits the rows of X and y together so that each compound’s feature vector stays paired with its activity label.</p></li>
<li><p>Creates four NumPy arrays</p>
<ul class="simple">
<li><p>X_train – feature matrix used to train the model</p></li>
<li><p>X_test – feature matrix held back for evaluation</p></li>
<li><p>y_train – activity labels for the training set</p></li>
<li><p>y_test – activity labels for the test set</p></li>
<li><p>idx_train  - row numbers of training samples</p></li>
<li><p>idx_test - row numbers of test samples</p></li>
</ul>
</li>
<li><p>Uses NumPy-style array slicing internally.
Which is why the outputs expose NumPy attributes like .shape.</p></li>
<li><p>Shuffles the data before splitting (shuffle=True).
This prevents ordering artifacts (e.g., all actives grouped together).</p></li>
<li><p>Preserves class balance (stratify=y).
The fraction of active vs. inactive compounds is maintained in both the training and test sets.</p></li>
<li><p>Ensures reproducibility (random_state=3100).
The same split will be generated every time this code is run.</p></li>
<li><p>Reserves 10% of the data for testing (test_size=0.1)
The remaining 90% is used for model training.</p></li>
<li><p>We removed CID, Index and SMILES and so there is no way to go back to which chemical is which, and idx_train/test does that</p></li>
</ol>
<p>In short, this single line converts your feature table and labels into four NumPy arrays structured exactly the way scikit-learn expects, establishing the data layout used throughout machine-learning workflows.                                                                     |</p>
</details>
</div>
</div>
<section id="save-the-test-train-split-as-np-arrays">
<h3>1.5.1 Save the test/train split as np arrays<a class="headerlink" href="#save-the-test-train-split-as-np-arrays" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">pathlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">Path</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">json</span>

<span class="c1"># Project root</span>
<span class="n">PROJECT_ROOT</span> <span class="o">=</span> <span class="n">Path</span><span class="o">.</span><span class="n">cwd</span><span class="p">()</span>  <span class="c1"># 10_ML</span>

<span class="c1"># Define split directory</span>
<span class="n">SPLIT_NAME</span> <span class="o">=</span> <span class="s2">&quot;90_10&quot;</span>
<span class="n">SPLITS</span> <span class="o">=</span> <span class="n">PROJECT_ROOT</span> <span class="o">/</span> <span class="s2">&quot;data&quot;</span> <span class="o">/</span> <span class="s2">&quot;AID743139&quot;</span> <span class="o">/</span> <span class="s2">&quot;splits&quot;</span> <span class="o">/</span> <span class="n">SPLIT_NAME</span>
<span class="n">SPLITS</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">parents</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Optional subfolder for arrays (keeps things tidy)</span>
<span class="n">ARRAYS</span> <span class="o">=</span> <span class="n">SPLITS</span> <span class="o">/</span> <span class="s2">&quot;arrays&quot;</span>
<span class="n">ARRAYS</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Save NumPy arrays</span>
<span class="n">np</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">ARRAYS</span> <span class="o">/</span> <span class="s2">&quot;X_train.npy&quot;</span><span class="p">,</span> <span class="n">X_train</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">ARRAYS</span> <span class="o">/</span> <span class="s2">&quot;X_test.npy&quot;</span><span class="p">,</span> <span class="n">X_test</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">ARRAYS</span> <span class="o">/</span> <span class="s2">&quot;y_train.npy&quot;</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">ARRAYS</span> <span class="o">/</span> <span class="s2">&quot;y_test.npy&quot;</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>

<span class="c1"># Save index mappings (critical for traceability)</span>
<span class="n">np</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">SPLITS</span> <span class="o">/</span> <span class="s2">&quot;train_idx.npy&quot;</span><span class="p">,</span> <span class="n">idx_train</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">SPLITS</span> <span class="o">/</span> <span class="s2">&quot;test_idx.npy&quot;</span><span class="p">,</span> <span class="n">idx_test</span><span class="p">)</span>

<span class="c1"># Save CID traceability</span>
<span class="n">df_data</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">idx_train</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;cid&quot;</span><span class="p">]]</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span>
    <span class="n">SPLITS</span> <span class="o">/</span> <span class="s2">&quot;train_cids.csv&quot;</span><span class="p">,</span>
    <span class="n">index</span><span class="o">=</span><span class="kc">False</span> 
<span class="p">)</span>

<span class="n">df_data</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">idx_test</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;cid&quot;</span><span class="p">]]</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span>
    <span class="n">SPLITS</span> <span class="o">/</span> <span class="s2">&quot;test_cids.csv&quot;</span><span class="p">,</span>
    <span class="n">index</span><span class="o">=</span><span class="kc">False</span>
<span class="p">)</span>

<span class="c1"># Save split metadata</span>
<span class="n">split_metadata</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;assay&quot;</span><span class="p">:</span> <span class="s2">&quot;AID743139&quot;</span><span class="p">,</span>
    <span class="s2">&quot;split_name&quot;</span><span class="p">:</span> <span class="n">SPLIT_NAME</span><span class="p">,</span>
    <span class="s2">&quot;description&quot;</span><span class="p">:</span> <span class="s2">&quot;Stratified 90/10 train/test split&quot;</span><span class="p">,</span>
    <span class="s2">&quot;test_fraction&quot;</span><span class="p">:</span> <span class="mf">0.10</span><span class="p">,</span>
    <span class="s2">&quot;train_fraction&quot;</span><span class="p">:</span> <span class="mf">0.90</span><span class="p">,</span>
    <span class="s2">&quot;stratified&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
    <span class="s2">&quot;random_state&quot;</span><span class="p">:</span> <span class="mi">3100</span><span class="p">,</span>
    <span class="s2">&quot;labels&quot;</span><span class="p">:</span> <span class="s2">&quot;Active vs Inactive&quot;</span><span class="p">,</span>
    <span class="s2">&quot;features&quot;</span><span class="p">:</span> <span class="s2">&quot;MACCS (variance-filtered)&quot;</span><span class="p">,</span>
    <span class="s2">&quot;source_feature_file&quot;</span><span class="p">:</span> <span class="s2">&quot;AID743139_MACCS_activites_noSalt_20260104_v1.csv&quot;</span>
<span class="p">}</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">SPLITS</span> <span class="o">/</span> <span class="s2">&quot;split_metadata.json&quot;</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">json</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">split_metadata</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[Saved] Train/test split → </span><span class="si">{</span><span class="n">SPLITS</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[Saved] Train/test split → /home/rebelford/jupyterbooks/cinf26book/content/modules/10_SupervisedML/data/AID743139/splits/90_10
</pre></div>
</div>
</div>
</div>
<p>The train/test split produces several files that serve different roles in the
machine-learning workflow. Some are used directly for model training, while
others preserve traceability, reproducibility, and interpretability. The table
below summarizes what was saved and why.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Purpose</strong></p></th>
<th class="head"><p><strong>Files</strong></p></th>
<th class="head"><p><strong>Why They Exist</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Model inputs</strong></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">X_train.npy</span></code>, <code class="docutils literal notranslate"><span class="pre">y_train.npy</span></code><br><code class="docutils literal notranslate"><span class="pre">X_test.npy</span></code>, <code class="docutils literal notranslate"><span class="pre">y_test.npy</span></code></p></td>
<td><p>Numerical feature matrices and label vectors used directly for model training and evaluation.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Operational split definition</strong></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">train_idx.npy</span></code>, <code class="docutils literal notranslate"><span class="pre">test_idx.npy</span></code></p></td>
<td><p>Exact row indices used to construct the split, allowing it to be verified, reconstructed, or reused.</p></td>
</tr>
<tr class="row-even"><td><p><strong>Chemical identity traceability</strong></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">train_cids.csv</span></code>, <code class="docutils literal notranslate"><span class="pre">test_cids.csv</span></code></p></td>
<td><p>Maps rows in the NumPy arrays back to compound identifiers for inspection, reporting, and auditing.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Provenance &amp; metadata</strong></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">split_metadata.json</span></code></p></td>
<td><p>Documents how the split was created, including parameters and source files, as part of the scientific record.</p></td>
</tr>
</tbody>
</table>
</div>
<p>Not all of these files are used immediately in this notebook. Some exist to support later model evaluation, interpretation, or reproducibility, and will be revisited in subsequent sections and notebooks.</p>
<div class="alert alert-block alert-success">
<strong>Data Artifacts Dive: What was Saved and Why</strong>
<p>Explain the role of the following files in the splits/90_10 directory. <strong>Be sure to open the files!</strong> You can open the *.csv and *.json files directly from the jupyter lab file browser.  You can open the *.npy files by executing the following in a code cell (and altering file name as appropriate).
<pre><code>
import numpy as np
from pathlib import Path
<p>PROJECT_ROOT = Path.cwd()  # 10_ML</p>
<p>SPLIT_NAME = “90_10”
SPLITS = PROJECT_ROOT / “data” / “AID743139” / “splits” / SPLIT_NAME
test_idx = np.load(SPLITS / “arrays/X_test.npy”)
test_idx
</code></pre>
The first three file types define the feature representation and labeling scheme used for model training and evaluation. They must be treated as a matched set.</p>
  <div style="
    background-color: #efffff;
    color: #000000;
    padding: 10px;
    border-radius: 4px;
    border: 1px solid #dddddd;
    margin-top: 10px;
  ">
<details>
    <summary>split_metadata.json</summary>
Documents how the split was performed
<ul>
    <li>Train/test ratio (90/10)</li>
    <li>Random seed used</li>
    <li>Apply stratification (maintain same active/inactive ratio of test and train sets as was in the original set</li>
    <li>Source feature file name</li>
</ul>
</details>
</div>
  <div style="
    background-color: #efffff;
    color: #000000;
    padding: 10px;
    border-radius: 4px;
    border: 1px solid #dddddd;
    margin-top: 10px;
  ">
<details>
    <summary>train_cids.csv and test_cids.csv</summary>
Identify which compounds (CIDs) ended up in test and traing set.
These files are for:
<ul>
    <li>Inspection</li>
    <li>Reporting</li>
    <li>Debugging</li>
</ul>
</details>
</div>
  <div style="
    background-color: #efffff;
    color: #000000;
    padding: 10px;
    border-radius: 4px;
    border: 1px solid #dddddd;
    margin-top: 10px;
  ">
<details>
    <summary>train_idx.npy and test_idx.npy</summary>
<p>
The train and test index files define the data split operationally by storing the exact row positions used to slice the feature matrix (X) and label vector (y). Applying these indices guarantees that every model, even when developed in different notebooks or with different algorithms, is trained and evaluated on the exact same compounds. Chemical identifiers such as CIDs are stored separately for interpretation and auditing, but the model itself only ever sees rows of numerical features selected by these indices.
<p>Once the split has been applied and saved as training and test arrays, the index files are no longer needed for routine model training or evaluation. They are retained as permanent artifacts to support traceability, auditing, and reproducibility, allowing the split to be verified, reconstructed, or reused if the feature representation is regenerated in the future.</p>
</p>
</details>
</div>
  <div style="
    background-color: #efffff;
    color: #000000;
    padding: 10px;
    border-radius: 4px;
    border: 1px solid #dddddd;
    margin-top: 10px;
  ">
<details>
    <summary>X_train.npy and y_train.npy</summary>
<p>
These files contain the training data used to build machine-learning models.
X_train.npy is a NumPy array representing the feature matrix, where each row corresponds to a compound and each column corresponds to a MACCS fingerprint bit retained after variance filtering. y_train.npy is the label vector, containing the binary activity labels (0 = inactive, 1 = active) for the same compounds, in the same row order.
<p>These arrays are the direct inputs to model fitting (e.g., model.fit(X_train, y_train)). The training data may later be balanced, weighted, or resampled, depending on the modeling strategy, but these files preserve the original, stratified train split exactly as defined.
To open and view the content of one of these file you can use the following command</p>
<pre><code>
import numpy as np
y_train = np.load("y_train.npy")
y_train[:5]
</code></pre>
<p>(Note: you have to set the path to the correct file location)</p>
</p>
</details>
</div>
  <div style="
    background-color: #efffff;
    color: #000000;
    padding: 10px;
    border-radius: 4px;
    border: 1px solid #dddddd;
    margin-top: 10px;
  ">
<details>
    <summary>X_test.npy and y_test.npy</summary>
<p>
These files contain the held-out test data used to evaluate model performance.
X_test.npy is the feature matrix for the test compounds, and y_test.npy contains their corresponding activity labels, aligned row-by-row with the feature matrix.
<p>The test arrays are never modified (no balancing or resampling) and are used only for prediction and evaluation (e.g., confusion matrices, ROC curves, precision/recall). Because they were created using a stratified split, they reflect the natural class distribution of the dataset and provide an unbiased assessment of model generalization.</p>
</p>
</details>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Write code here to view a *.npy files in the arrays folder (see above hints)</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Looking Ahead: Preparing Data for Model Building</strong></p>
<p>In the next section, we will reload these saved training and test arrays and
begin preparing the training data for model construction. One common challenge
in biological activity datasets is class imbalance, where the number of
inactive compounds greatly exceeds the number of active compounds. When left
unaddressed, this imbalance can cause a model to favor the dominant (majority)
class rather than learning meaningful patterns associated with activity.</p>
<p>We will explore strategies for addressing this issue using only the training
data, while keeping the test set unchanged so that model evaluation remains
unbiased.</p>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="preparing-the-training-data-model-construction">
<h1>2. Preparing the Training Data Model Construction<a class="headerlink" href="#preparing-the-training-data-model-construction" title="Link to this heading">#</a></h1>
<p>In this section, we focus exclusively on the training data and the preprocessing steps that are permitted during model construction.
Unlike earlier steps, where we defined the feature space and created a fixed train–test split, the operations introduced here may intentionally modify the training data to help the model learn more effectively.</p>
<p>A key principle governs all steps in this section: <strong>the test set is never altered</strong>. Any transformations, resampling, or adjustments are applied only to the training data, ensuring that model evaluation remains unbiased and reflects performance on unseen data.</p>
<section id="reload-the-saved-training-and-test-arrays">
<h2>2.1 Reload the Saved Training and Test Arrays<a class="headerlink" href="#reload-the-saved-training-and-test-arrays" title="Link to this heading">#</a></h2>
<p>If you are continuing directly from Section 1 without restarting the kernel, the training and test arrays may already be in memory. However, it is important that you know how to reload these arrays from disk in case the kernel has been restarted or you are returning to the notebook at a later time. For that reason, we begin this section by explicitly loading the saved NumPy arrays. If the arrays are already in memory, you may treat this step as a demonstration of how the data can be recovered when needed.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">pathlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">Path</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="c1"># Define project root (10_ML)</span>
<span class="n">PROJECT_ROOT</span> <span class="o">=</span> <span class="n">Path</span><span class="o">.</span><span class="n">cwd</span><span class="p">()</span>

<span class="c1"># Define arrays directory</span>
<span class="n">ARRAY_DIR</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">PROJECT_ROOT</span>
    <span class="o">/</span> <span class="s2">&quot;data&quot;</span>
    <span class="o">/</span> <span class="s2">&quot;AID743139&quot;</span>
    <span class="o">/</span> <span class="s2">&quot;splits&quot;</span>
    <span class="o">/</span> <span class="s2">&quot;90_10&quot;</span>
    <span class="o">/</span> <span class="s2">&quot;arrays&quot;</span>
<span class="p">)</span>

<span class="c1"># Load arrays into active memory</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">ARRAY_DIR</span> <span class="o">/</span> <span class="s2">&quot;X_train.npy&quot;</span><span class="p">)</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">ARRAY_DIR</span> <span class="o">/</span> <span class="s2">&quot;y_train.npy&quot;</span><span class="p">)</span>

<span class="n">X_test</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">ARRAY_DIR</span> <span class="o">/</span> <span class="s2">&quot;X_test.npy&quot;</span><span class="p">)</span>
<span class="n">y_test</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">ARRAY_DIR</span> <span class="o">/</span> <span class="s2">&quot;y_test.npy&quot;</span><span class="p">)</span>

<span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y_train</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>((6113, 162), (6113,))
</pre></div>
</div>
</div>
</div>
<div class="alert alert-block alert-info">
<strong>What does it mean to load a Numpy Array</strong>
<p>The following code loads two objects into memory. Explain what these objects are, where they come from, and how they are used in the machine-learning workflow.
</p>
<pre><code>X_train = np.load(ARRAY_DIR / "X_train.npy")
y_train = np.load(ARRAY_DIR / "y_train.npy")</code></pre>
  <div style="
    background-color: #efffff;
    color: #000000;
    padding: 10px;
    border-radius: 4px;
    border: 1px solid #dddddd;
    margin-top: 10px;
  ">
<details>
    <summary>Answer</summary>
 <p>
    The files <code>X_train.npy</code> and <code>y_train.npy</code> store NumPy arrays
    on disk. These files are <em>persistent</em>, meaning they remain available even
    after the kernel is restarted or the notebook is closed.
  </p>
  <p>
    When <code>np.load()</code> is called, the data in these files are read from disk
    and loaded into active memory as the variables <code>X_train</code> and
    <code>y_train</code>. At this point, they are ordinary NumPy arrays
    (<code>numpy.ndarray</code>) that can be passed directly to machine-learning
    algorithms.
  </p>
  <p>
    Although one representation exists on disk (nonvolatile) and the other exists in
    memory (volatile), they are structurally identical. Once loaded, the model cannot
    distinguish whether a NumPy array was created by computation or loaded from a file.
  </p>
</details>
</div>
</div></section>
<section id="examine-class-imbalance">
<h2>2.2 Examine class imbalance<a class="headerlink" href="#examine-class-imbalance" title="Link to this heading">#</a></h2>
<section id="what-is-class-imbalance-and-why-it-matters">
<h3>2.2.1 What is class imbalance and why it matters<a class="headerlink" href="#what-is-class-imbalance-and-why-it-matters" title="Link to this heading">#</a></h3>
<p>Before training a classification model, it is important to examine the distribution of class labels in the training set. In many biological datasets, the number of inactive compounds far exceeds the number of active compounds. This situation is known as class imbalance.</p>
<p>When class imbalance is present, a model may appear to perform well overall while failing to correctly identify the minority class. For example, a model trained on highly imbalanced data may learn to predict the majority class (typically inactives) most of the time, resulting in misleading accuracy and poor detection of the minority class (typically active compounds). By quantifying the class distribution early, we can assess the severity of imbalance and decide whether additional steps are needed to support effective model learning.</p>
<div class="alert alert-block alert-success"> 
<strong>Deeper Dive:</strong> Strategies for Handling Imbalanced Data
  <div style="
    background-color: #efffff;
    color: #000000;
    padding: 10px;
    border-radius: 4px;
    border: 1px solid #dddddd;
    margin-top: 10px;
  ">
<details>
    <summary>Strategies</summary>
<p>Many classification problems involve <strong>imbalanced datasets</strong>, where one class occurs much more frequently than another.</p>
<p>If untreated, imbalance can cause models to:</p>
<ul class="simple">
<li><p>Favor the majority class</p></li>
<li><p>Misclassify rare but important samples</p></li>
<li><p>Produce misleading accuracy metrics</p></li>
</ul>
<hr class="docutils" />
<p><strong>Common strategies</strong></p>
<p>There are three broad approaches to handling class imbalance:</p>
<p><strong>1. Downsampling (Under-sampling)</strong></p>
<ul class="simple">
<li><p>Randomly remove samples from the majority class</p></li>
<li><p>Simple and effective</p></li>
<li><p>Risk: loss of potentially useful information</p></li>
</ul>
<p><strong>2. Oversampling</strong></p>
<ul class="simple">
<li><p>Increase the size of the minority class</p></li>
<li><p>Can be done by duplication or synthetic methods (e.g., SMOTE)</p></li>
<li><p>Risk: overfitting to replicated or artificial samples</p></li>
</ul>
<p><strong>3. Cost-sensitive learning</strong></p>
<ul class="simple">
<li><p>Penalize misclassification of the minority class more heavily</p></li>
<li><p>Often implemented via class weights</p></li>
<li><p>Keeps all data but changes the learning objective</p></li>
</ul>
<hr class="docutils" />
<p><strong>Key takeaway</strong></p>
<p>Handling class imbalance is not about maximizing accuracy; it is about shaping the decision boundary so that the assignment of class labels y to feature vectors X is driven by meaningful feature patterns, not by class frequency alone.</p>
</details>
</div>
</div>
</section>
<section id="measuring-class-imbalance-in-the-training-set">
<h3>2.2.2 Measuring Class Imbalance in the Training Set<a class="headerlink" href="#measuring-class-imbalance-in-the-training-set" title="Link to this heading">#</a></h3>
<p>To understand the severity of class imbalance in this dataset, we examine the number of inactive and active compounds in the training set and compute their ratio.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;# inactives in training set: &quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span> <span class="o">-</span> <span class="n">y_train</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;# actives in training set:   &quot;</span><span class="p">,</span> <span class="n">y_train</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>
<span class="n">ratio</span> <span class="o">=</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span> <span class="o">-</span> <span class="n">y_train</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span><span class="o">/</span><span class="n">y_train</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;the ratio of inactive to active in training set=&quot;</span><span class="p">,</span> <span class="n">ratio</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span># inactives in training set:  5444
# actives in training set:    669
the ratio of inactive to active in training set= 8.137518684603886
</pre></div>
</div>
</div>
</div>
<div class="alert alert-block alert-info">
<details>
<summary>How to interpret class balance ratios.</summary>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-right"><p><strong>Ratio</strong></p></th>
<th class="head"><p><strong>How to Interpret the Value</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-right"><p>1</p></td>
<td><p>Balanced: Roughly equal number of active and inactive. Ideal for training</p></td>
</tr>
<tr class="row-odd"><td class="text-right"><p>2</p></td>
<td><p>Mild imbalance: 1 active for every 2 inactives. Still manageable, but performance of minority class should be monitored.</p></td>
</tr>
<tr class="row-even"><td class="text-right"><p>5</p></td>
<td><p>Severe imbalance. Model may predict majority class most of the time and ignore the minority class.</p></td>
</tr>
</tbody>
</table>
</div>
<p>Before applying any balancing strategy, it is important to keep the following workflow principles in mind:</p>
<ul class="simple">
<li><p>Always save unbalanced train/test splits</p></li>
<li><p>Never balance the test set</p></li>
<li><p>Treat balancing as part of the model, not the data</p></li>
<li><p>Expect some variability when downsampling</p></li>
<li><p>Use fixed seeds when debugging, variable seeds when evaluating robustness</p></li>
</ul>
</section>
</section>
<section id="balance-the-training-set-by-downsampling">
<h2>2.3 Balance the Training Set by Downsampling<a class="headerlink" href="#balance-the-training-set-by-downsampling" title="Link to this heading">#</a></h2>
<p>When a dataset exhibits strong class imbalance, one common mitigation strategy is downsampling the majority class. In downsampling, we randomly select a subset of the majority class so that its size is comparable to that of the minority class. This encourages the model to learn decision boundaries that treat both classes more equitably, rather than defaulting to predictions driven primarily by class frequency.</p>
<p>Downsampling reduces the total amount of training data and may discard potentially informative samples from the majority class. However, it often improves a model’s ability to recognize the minority class and reduces bias toward the dominant class. In this notebook, we use downsampling as a simple, transparent strategy to address class imbalance during model training.</p>
<p>Importantly, this procedure is applied only to the training data. The test set remains unchanged and continues to reflect the original class distribution.</p>
</section>
<section id="id1">
<h2>2.3 Balance the training set by downsampling<a class="headerlink" href="#id1" title="Link to this heading">#</a></h2>
<p>When a dataset has a strong class imbalance a common mitigation strategy is downsampling the majority class. In downsampling, we randomly select a subset of the majority class so that its size is comparable to that of the minority class. This encourages the model to learn decision boundaries that treat both classes more equitably, rather than defaulting to predictions dominated by the majority class. While downsampling reduces the total amount of training data and may discard potentially informative samples from the majority class, it often improves model performance on the minority class and reduces bias toward the majority class.</p>
<p>In the code cell below, we randomly select a subset of the inactive compounds such that their number matches the number of active compounds.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Indices of each class</span>
<span class="n">idx_inactives</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">y_train</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">idx_actives</span>   <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">y_train</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># Number of observations in each class</span>
<span class="n">num_inactives</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">idx_inactives</span><span class="p">)</span>
<span class="n">num_actives</span>   <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">idx_actives</span><span class="p">)</span>

<span class="c1"># Downsample inactives to match number of actives</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">idx_inactives_downsampled</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span>
    <span class="n">idx_inactives</span><span class="p">,</span>
    <span class="n">size</span><span class="o">=</span><span class="n">num_actives</span><span class="p">,</span>
    <span class="n">replace</span><span class="o">=</span><span class="kc">False</span>
<span class="p">)</span>

<span class="c1"># Create balanced training set</span>
<span class="n">X_train_bal</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span>
    <span class="n">X_train</span><span class="p">[</span><span class="n">idx_inactives_downsampled</span><span class="p">],</span>
    <span class="n">X_train</span><span class="p">[</span><span class="n">idx_actives</span><span class="p">]</span>
<span class="p">))</span>

<span class="n">y_train_bal</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span>
    <span class="n">y_train</span><span class="p">[</span><span class="n">idx_inactives_downsampled</span><span class="p">],</span>
    <span class="n">y_train</span><span class="p">[</span><span class="n">idx_actives</span><span class="p">]</span>
<span class="p">))</span>

<span class="c1"># Confirm balancing worked</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;# inactives:&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_train_bal</span><span class="p">)</span> <span class="o">-</span> <span class="n">y_train_bal</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;# actives:  &quot;</span><span class="p">,</span> <span class="n">y_train_bal</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>

<span class="n">ratio</span> <span class="o">=</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_train_bal</span><span class="p">)</span> <span class="o">-</span> <span class="n">y_train_bal</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span> <span class="o">/</span> <span class="n">y_train_bal</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;ratio inactive to active =&quot;</span><span class="p">,</span> <span class="n">ratio</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Training set shape:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;X:&quot;</span><span class="p">,</span> <span class="n">X_train_bal</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;y:&quot;</span><span class="p">,</span> <span class="n">y_train_bal</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span># inactives: 669
# actives:   669
ratio inactive to active = 1.0

Training set shape:
X: (1338, 162)
y: (1338,)
</pre></div>
</div>
</div>
</div>
<div class="alert alert-block alert-info">
<strong>Check Your Understanding: Interpreting the Effect of Balancing</strong>
<p>
The following is <em>sample output</em> from this dataset before and after
downsampling. Your exact values may differ if a different random seed or
dataset version is used.
</p>
<table style="border-collapse: collapse; width: 100%; margin-top: 10px;">
  <thead>
    <tr style="background-color: #f2f2f2;">
      <th style="border: 1px solid #cccccc; padding: 8px;">Training Set</th>
      <th style="border: 1px solid #cccccc; padding: 8px;"># Inactives</th>
      <th style="border: 1px solid #cccccc; padding: 8px;"># Actives</th>
      <th style="border: 1px solid #cccccc; padding: 8px;">Inactive : Active Ratio</th>
      <th style="border: 1px solid #cccccc; padding: 8px;">X Shape</th>
      <th style="border: 1px solid #cccccc; padding: 8px;">y Shape</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="border: 1px solid #cccccc; padding: 8px;">
        Original (Unbalanced)
      </td>
      <td style="border: 1px solid #cccccc; padding: 8px;">5444</td>
      <td style="border: 1px solid #cccccc; padding: 8px;">669</td>
      <td style="border: 1px solid #cccccc; padding: 8px;">≈ 8.1 : 1</td>
      <td style="border: 1px solid #cccccc; padding: 8px;">
        (6113, 162)
      </td>
      <td style="border: 1px solid #cccccc; padding: 8px;">
        (6113,)
      </td>
    </tr>
    <tr>
      <td style="border: 1px solid #cccccc; padding: 8px;">
        Balanced (Downsampled)
      </td>
      <td style="border: 1px solid #cccccc; padding: 8px;">669</td>
      <td style="border: 1px solid #cccccc; padding: 8px;">669</td>
      <td style="border: 1px solid #cccccc; padding: 8px;">1 : 1</td>
      <td style="border: 1px solid #cccccc; padding: 8px;">
        (1338, 162)
      </td>
      <td style="border: 1px solid #cccccc; padding: 8px;">
        (1338,)
      </td>
    </tr>
  </tbody>
</table>
Answer the Following Questions:
<p>1. What was the original set balanced or unbalanced and identify the majority and minority classes </p>
<div style="
  background-color: #efffff;
  color: #000000;
  padding: 10px;
  border-radius: 4px;
  border: 1px solid #dddddd;
  margin-top: 10px;
">
<details>
<summary>Answer</summary>
<p>
The initial data set was highly imbalanced with 8 Inactives (majority class) for each active (minority class).
</p>
</details>
</div>
<p>2. Explain from the data how downsampling worked</p>
<div style="
  background-color: #efffff;
  color: #000000;
  padding: 10px;
  border-radius: 4px;
  border: 1px solid #dddddd;
  margin-top: 10px;
">
<details>
<summary>Answer</summary>
<p>
Initially there were 5,444 inactives and 669 actives in the dataset.  The downsampling reduced the number of unbalanced to the number of balanced, giving 669 compounds of each class, with a total dataset of 1338 compounds.  
</p>
</details>
</div>
<p>3. Can you explain the shape of the feature matrix and label vector and what data they contain?</p>
<div style="
  background-color: #efffff;
  color: #000000;
  padding: 10px;
  border-radius: 4px;
  border: 1px solid #dddddd;
  margin-top: 10px;
">
<details>
<summary>Answer</summary>
<p>
<ul>
  <li>The feature matrix</li>
    <ul>
      <li>Contains 162 columns representing the MACCS fingerprints after the zero-variance mask was applied.</li>
       <li>Each row represents a compound</li>
       <li>The original training se contains 6113 compounds</li>
       <li>The balanced (downsampled) training set contains 1338 compounds</li>
    </ul>
</li>
    <li> The label vector contains the Boolean value to indicate a compound is active (1) or inactive (0)</li>
</ul>    
</p>
</details>
</div>
<p>4. Do the balanced feature matrix (<code>X_train_bal</code>) or label vector
(<code>y_train_bal</code>) contain the chemical identity (e.g., CID) of each
compound? If not, why is this information absent, and how could the chemical
identity of a specific row be recovered if needed?</p>
<div style="
  background-color: #efffff;
  color: #000000;
  padding: 10px;
  border-radius: 4px;
  border: 1px solid #dddddd;
  margin-top: 10px;
">
<details>
<summary>Answer</summary>
<p>
No. Neither the feature matrix (<code>X_train_bal</code>) nor the label vector (<code>y_train_bal</code>) contains explicit chemical identity information such as CIDs or SMILES strings. These NumPy arrays store only numerical data: fingerprint bits in <code>X</code> and binary activity labels in <code>y</code>.
</p>
<p>
This separation is intentional. Machine-learning models operate on numerical feature vectors and labels, not on compound identifiers. Including chemical identity directly in the feature or label arrays would mix metadata with model inputs and violate standard machine-learning design principles.
</p>
<p>
Chemical identity is instead preserved in separate artifacts created earlier in section 1.5.1 of this workflow, such as the <code>train_cids.csv</code> file and the corresponding index arrays (e.g., <code>train_idx.npy</code>). These files allow individual
rows in the NumPy arrays to be traced back to specific compounds without embedding identity information into the model’s inputs.
</p>
<p>
This design cleanly separates <em>what the model learns</em> (numerical feature patterns) from <em>how results are interpreted</em> (mapping predictions back to chemical compounds), and supports reproducibility, auditing, and downstream analysis.
</p>
</details>
</div>
</div>
<div class="alert alert-block alert-success">
<strong>Code Dive:</strong> How Downsampling Works with NumPy
<div style="
  background-color: #efffff;
  color: #000000;
  padding: 10px;
  border-radius: 4px;
  border: 1px solid #dddddd;
  margin-top: 10px;
">
<details>
<summary>Explanation</summary>
<p>
This code performs downsampling using NumPy array operations rather than
Pandas DataFrames. The goal is to construct a new, balanced training set
from the original training data.
</p>
<p><b>Identifying class membership</b></p>
<p>
The calls to <code>np.where()</code> locate the row indices corresponding to
inactive (<code>y = 0</code>) and active (<code>y = 1</code>) compounds in the
training label vector.
</p>
<pre><code>
idx_inactives = np.where(y_train == 0)[0]
idx_actives   = np.where(y_train == 1)[0]
</code></pre>
<p><b>Random downsampling of the majority class</b></p>
<p>
A subset of inactive indices is randomly selected without replacement so
that the number of inactive samples matches the number of active samples.
</p>
<pre><code>
idx_inactives_downsampled = np.random.choice(
    idx_inactives,
    size=num_actives,
    replace=False
)
</code></pre>
<p><b>Reassembling the balanced training arrays</b></p>
<p>
The balanced feature matrix and label vector are constructed by stacking
the selected inactive samples together with all active samples.
</p>
<pre><code>
X_train_bal = np.vstack((
    X_train[idx_inactives_downsampled],
    X_train[idx_actives]
))

y_train_bal = np.hstack((
    y_train[idx_inactives_downsampled],
    y_train[idx_actives]
))
</code></pre>
<p>
The result is a balanced training dataset in which both classes are equally
represented and aligned row-by-row.
</p>
</details>
</div>
</div>
<p><strong>Summary: Imbalanced vs. Balanced Training Data</strong></p>
<p>At this point, we have two versions of the training data:</p>
<ul class="simple">
<li><p>The original training set, which reflects the natural class imbalance of   the dataset.</p></li>
<li><p>A balanced training set, created by downsampling the majority class.</p></li>
</ul>
<p>Both versions are valid representations of the training data, but they serve different purposes. The imbalanced training set preserves the original data distribution, while the balanced training set emphasizes equal representation of both classes during model learning.</p>
<p>In the next section, we will build and evaluate a classification model using the balanced training data and compare its behavior to models trained on unbalanced data. This comparison will help illustrate how class imbalance and balancing strategies influence model predictions and performance.</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="build-a-model-using-the-training-set">
<h1>3. Build a model using the training set.<a class="headerlink" href="#build-a-model-using-the-training-set" title="Link to this heading">#</a></h1>
<p>Now we are ready to build predictive models using machine learning algorithms available in the scikit-learn library (https://scikit-learn.org/). This notebook will use Naïve Bayes it is relatively fast and simple.</p>
<section id="naive-bayes">
<h2>3.1 Naïve Bayes<a class="headerlink" href="#naive-bayes" title="Link to this heading">#</a></h2>
<p>Naïve Bayes is a family of probabilistic classification algorithms that are particularly well suited for datasets where features are represented as binary indicators. In cheminformatics, this commonly occurs when molecules are encoded using molecular fingerprints, where each feature answers a yes/no question, such as whether a specific substructure is present in the molecule. Bernoulli Naïve Bayes is the variant designed specifically for this situation. It treats each fingerprint bit as a binary feature (0 or 1) and learns how often each feature appears in each class (for example, active versus inactive compounds). During prediction, the model combines this information across all features to estimate which class best matches the observed pattern of fingerprint bits.</p>
<p>A simplifying assumption made by Naïve Bayes is that each feature contributes independently to the classification decision. In other words, the model treats each fingerprint bit as providing its own piece of evidence, without explicitly accounting for relationships between bits. This assumption is more reasonable for some types of molecular fingerprints than others. For example, MACCS keys are based on a fixed set of predefined structural patterns, and many of these bits represent distinct chemical features. In contrast, Morgan fingerprints are generated from overlapping atomic environments, so multiple bits may be activated by the same underlying substructure. As a result, Morgan fingerprint bits tend to be more strongly correlated with one another.</p>
<p>In practice, the independence assumption is rarely strictly true,especially in chemistry, where molecular features are inherently related. However, Naïve Bayes often performs well despite this simplification, particularly for high-dimensional, sparse feature vectors such as molecular fingerprints. The algorithm’s simplicity can make it surprisingly robust, even when its assumptions are only approximately satisfied.</p>
<p>In addition to modeling how features behave within each class, Naïve Bayes also incorporates how common each class is in the training data. When the dataset is balanced, the model treats each class as equally plausible before considering any molecular features, so classification decisions are driven primarily by how well the fingerprint pattern matches each class. When the dataset is imbalanced, the model naturally favors the more common class unless the feature evidence strongly supports the alternative. This interaction between feature evidence (X) and class prevalence (y) plays an important role in how Naïve Bayes behaves and motivates strategies such as downsampling.</p>
<p>Please see <strong><a class="reference external" href="https://rebelford.github.io/2026Cheminformatics/content/appendices/App_10/A_10-2NaiveBayes.html">Appendix 10.2” Bayes’s Theorem: From Inference to Models</a></strong> for a workup on how Naïve Bayes works.</p>
<div class="alert alert-block alert-success"> 
<strong>Deeper Dive:</strong> How Bernoulli Naïve Bayes Works (Conceptual Math)
  <div style="
    background-color: #efffff;
    color: #000000;
    padding: 10px;
    border-radius: 4px;
    border: 1px solid #dddddd;
    margin-top: 10px;
  ">
<details>
    <summary>Explanation</summary>
<p>Bernoulli Naïve Bayes is a probabilistic model, meaning it uses probabilities to decide which class label is most consistent with the observed features. At a high level, the model answers the question: “Given the fingerprint bits for this molecule, which class is more likely, 1 (active) or 0 (inactive)?”</p>
<p><strong>The core idea: updating beliefs</strong></p>
<p>The model starts with a baseline expectation about each class (for example, how common active compounds are overall) and then updates that expectation based on the observed fingerprint bits. This idea is formalized by Bayes’ Theorem:</p>
<div class="math notranslate nohighlight">
\[
P(y \mid X) = \frac{P(X \mid y)\, P(y)}{P(X)}
\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( y \)</span> is the class label (e.g., active or inactive)</p></li>
<li><p><span class="math notranslate nohighlight">\( X \)</span> represents all fingerprint bits for a molecule</p></li>
<li><p><span class="math notranslate nohighlight">\( P(y \mid X) \)</span> is the probability of class ( y ) given the observed features</p></li>
</ul>
<p>In practice, the model compares this quantity across classes and selects the most likely one.</p>
<hr class="docutils" />
<p><strong>What each term means (intuitively)</strong></p>
<p><strong><span class="math notranslate nohighlight">\( P(y) \)</span> — the prior</strong><br />
This represents how common each class is <em>before</em> looking at any molecular features. If most compounds are inactive, the prior probability of “inactive” is higher.</p>
<p><strong><span class="math notranslate nohighlight">\( P(X \mid y) \)</span> — the likelihood</strong><br />
This measures how well the observed fingerprint bits match what the model has learned about class ( y ). This is where the fingerprint information is used.</p>
<p><strong><span class="math notranslate nohighlight">\( P(X) \)</span> — the evidence</strong><br />
This term ensures probabilities are properly scaled. Because it is the same for all classes, it does not affect which class is chosen and is usually ignored during classification.</p>
<hr class="docutils" />
<p><strong>The “naïve” assumption: breaking features apart</strong></p>
<p>Computing <span class="math notranslate nohighlight">\( P(X \mid y) \)</span> directly would be extremely difficult for molecular fingerprints, because many bits are related to one another. Naïve Bayes simplifies this by assuming that each fingerprint bit can be treated independently when estimating probabilities. This allows the likelihood to be written as a product:</p>
<div class="math notranslate nohighlight">
\[
P(X \mid y) = \prod_i P(x_i \mid y)
\]</div>
<p>Each fingerprint bit contributes its own small piece of evidence toward the final decision. Together, these pieces of evidence are combined to determine which class the molecule most closely resembles overall.</p>
<hr class="docutils" />
<p><strong>Why it is called <em>Bernoulli</em> Naïve Bayes</strong>
In Bernoulli Naïve Bayes, each fingerprint bit is treated as a Bernoulli random variable, meaning it can take only two values:</p>
<ul class="simple">
<li><p>Value = 1 → feature is present</p></li>
<li><p>Value = 0 → feature is absent</p></li>
</ul>
<p>The class label <span class="math notranslate nohighlight">\( 𝑦 \)</span> also takes on discrete values (for example, 0 = inactive and 1 = active). During training, the model looks at all molecules in each class and learns how often each fingerprint bit is present or absent within that class. For each bit and each class, the model learns a probability such as:</p>
<div class="math notranslate nohighlight">
\[
P(x_i = 1 \mid y)
\]</div>
<p>The above equation is saying that, among molecules in class <span class="math notranslate nohighlight">\(y\)</span>, how common it is for bit <span class="math notranslate nohighlight">\(i\)</span> to be present (<span class="math notranslate nohighlight">\(=1\)</span>).</p>
<p>During prediction:</p>
<ul class="simple">
<li><p>If a bit is present, the model uses the probability of presence</p></li>
<li><p>If a bit is absent, the model uses the probability of absence</p></li>
</ul>
<p>These probabilities are multiplied together across all bits to form an overall score for each class.</p>
<hr class="docutils" />
<p><strong>Why the math still works in practice</strong></p>
<p>Even though fingerprint bits are not truly independent—especially for Morgan fingerprints—the model often performs well because:</p>
<ul class="simple">
<li><p>Many weak signals combine into a strong overall pattern</p></li>
<li><p>Errors introduced by the independence assumption tend to cancel out</p></li>
<li><p>Classification depends on relative likelihoods, not exact probabilities</p></li>
</ul>
<p>As a result, Naïve Bayes often provides good classification performance even when its assumptions are only approximately true.</p>
<hr class="docutils" />
<p>For a more detailed explaination go to <strong><a href="https://rebelford.github.io/2026Cheminformatics/content/appendices/App_10/A_10-2NaiveBayes.html">Appendix A10.2 Bayes’ Theorem: From Inference to Models</a></strong></p>
</details>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.naive_bayes</span><span class="w"> </span><span class="kn">import</span> <span class="n">BernoulliNB</span>        <span class="c1">#-- Naïve Bayes</span>

<span class="c1"># set up the NB classification model. Bernoulli is specific for binary features (0,1) </span>
<span class="n">clf_NB</span> <span class="o">=</span> <span class="n">BernoulliNB</span><span class="p">()</span>  
</pre></div>
</div>
</div>
</div>
<p>In scikit-learn examples, the prefix <code class="docutils literal notranslate"><span class="pre">clf</span></code> is commonly used as shorthand for <em>classifier</em>. In this notebook, <code class="docutils literal notranslate"><span class="pre">clf_NB</span></code> refers to a Bernoulli Naïve Bayes classifier object. This naming convention is descriptive only; it is not required by Python or by scikit-learn. Its purpose is to make the role of the object explicit within the machine-learning workflow.</p>
<p>At this point in the course, two perspectives are coming together:</p>
<ol class="arabic simple">
<li><p>The scientific/statistical model, introduced conceptually in Appendix A-10.2</p></li>
<li><p>The scikit-learn implementation, which represents that model as a Python object with a defined interface and behavior</p></li>
</ol>
<p>The <code class="docutils literal notranslate"><span class="pre">BernoulliNB</span></code> class in scikit-learn is a concrete implementation of the Naïve Bayes probabilistic model described in the Appendix.</p>
<p>The call
<code class="docutils literal notranslate"><span class="pre">clf_NB.fit(X_train_balanced,</span> <span class="pre">y_train_balanced)</span></code>
<em>trains</em> the Bernoulli Naïve Bayes model using the training data. In scikit-learn terminology, <em>fitting</em> a model means <em>estimating the parameters of the statistical model from the data</em> and storing them inside the classifier object.</p>
<p>During training, the classifier learns how molecular feature vectors are associated with activity labels. Specifically, it estimates two categories of quantities from the training set:</p>
<ul class="simple">
<li><p><strong>Feature–class likelihoods</strong>
For each feature (each column in <code class="docutils literal notranslate"><span class="pre">X_train_balanced</span></code>) and for each class label in <code class="docutils literal notranslate"><span class="pre">y_train_balanced</span></code>, the model estimates how frequently that feature occurs among samples belonging to that class. Because Bernoulli Naïve Bayes is designed for binary features, this corresponds to learning how often each fingerprint bit is equal to 1 within each class.</p></li>
<li><p><strong>Class prior probabilities</strong>
The model also estimates how common each class is in the training data. These class frequencies form the <em>prior probabilities</em>, which represent how likely each class is before considering any molecular features.</p></li>
</ul>
<p>Together, these learned quantities define the decision rule used by the classifier. After training is complete, the model has all the information required to evaluate new molecular feature vectors. At this stage, the model is not yet making predictions. Instead, it is establishing and storing the statistical relationships that will later be used during evaluation and inference.</p>
<p>Conceptually, the trained <code class="docutils literal notranslate"><span class="pre">BernoulliNB</span></code> object is the computational realization of the probabilistic model described in Appendix A-10.2. Once trained, it exposes two primary interfaces:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">predict_proba</span></code>, which returns class probabilities</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">predict</span></code>, which returns discrete class assignments</p></li>
</ul>
<p>These interfaces will be used explicitly in later sections when we evaluate model performance and apply the model to new data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Train the model by fitting it to the data.</span>
<span class="n">clf_NB</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span> <span class="n">X_train_bal</span><span class="p">,</span> <span class="n">y_train_bal</span><span class="p">)</span> 
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Model expects features:&quot;</span><span class="p">,</span> <span class="n">clf_NB</span><span class="o">.</span><span class="n">n_features_in_</span><span class="p">)</span>
<span class="n">clf_NB</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model expects features: 162
</pre></div>
</div>
<div class="output text_html"><style>#sk-container-id-1 {
  /* Definition of color scheme common for light and dark mode */
  --sklearn-color-text: #000;
  --sklearn-color-text-muted: #666;
  --sklearn-color-line: gray;
  /* Definition of color scheme for unfitted estimators */
  --sklearn-color-unfitted-level-0: #fff5e6;
  --sklearn-color-unfitted-level-1: #f6e4d2;
  --sklearn-color-unfitted-level-2: #ffe0b3;
  --sklearn-color-unfitted-level-3: chocolate;
  /* Definition of color scheme for fitted estimators */
  --sklearn-color-fitted-level-0: #f0f8ff;
  --sklearn-color-fitted-level-1: #d4ebff;
  --sklearn-color-fitted-level-2: #b3dbfd;
  --sklearn-color-fitted-level-3: cornflowerblue;

  /* Specific color for light theme */
  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));
  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-icon: #696969;

  @media (prefers-color-scheme: dark) {
    /* Redefinition of color scheme for dark theme */
    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));
    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-icon: #878787;
  }
}

#sk-container-id-1 {
  color: var(--sklearn-color-text);
}

#sk-container-id-1 pre {
  padding: 0;
}

#sk-container-id-1 input.sk-hidden--visually {
  border: 0;
  clip: rect(1px 1px 1px 1px);
  clip: rect(1px, 1px, 1px, 1px);
  height: 1px;
  margin: -1px;
  overflow: hidden;
  padding: 0;
  position: absolute;
  width: 1px;
}

#sk-container-id-1 div.sk-dashed-wrapped {
  border: 1px dashed var(--sklearn-color-line);
  margin: 0 0.4em 0.5em 0.4em;
  box-sizing: border-box;
  padding-bottom: 0.4em;
  background-color: var(--sklearn-color-background);
}

#sk-container-id-1 div.sk-container {
  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`
     but bootstrap.min.css set `[hidden] { display: none !important; }`
     so we also need the `!important` here to be able to override the
     default hidden behavior on the sphinx rendered scikit-learn.org.
     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */
  display: inline-block !important;
  position: relative;
}

#sk-container-id-1 div.sk-text-repr-fallback {
  display: none;
}

div.sk-parallel-item,
div.sk-serial,
div.sk-item {
  /* draw centered vertical line to link estimators */
  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));
  background-size: 2px 100%;
  background-repeat: no-repeat;
  background-position: center center;
}

/* Parallel-specific style estimator block */

#sk-container-id-1 div.sk-parallel-item::after {
  content: "";
  width: 100%;
  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);
  flex-grow: 1;
}

#sk-container-id-1 div.sk-parallel {
  display: flex;
  align-items: stretch;
  justify-content: center;
  background-color: var(--sklearn-color-background);
  position: relative;
}

#sk-container-id-1 div.sk-parallel-item {
  display: flex;
  flex-direction: column;
}

#sk-container-id-1 div.sk-parallel-item:first-child::after {
  align-self: flex-end;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:last-child::after {
  align-self: flex-start;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:only-child::after {
  width: 0;
}

/* Serial-specific style estimator block */

#sk-container-id-1 div.sk-serial {
  display: flex;
  flex-direction: column;
  align-items: center;
  background-color: var(--sklearn-color-background);
  padding-right: 1em;
  padding-left: 1em;
}


/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is
clickable and can be expanded/collapsed.
- Pipeline and ColumnTransformer use this feature and define the default style
- Estimators will overwrite some part of the style using the `sk-estimator` class
*/

/* Pipeline and ColumnTransformer style (default) */

#sk-container-id-1 div.sk-toggleable {
  /* Default theme specific background. It is overwritten whether we have a
  specific estimator or a Pipeline/ColumnTransformer */
  background-color: var(--sklearn-color-background);
}

/* Toggleable label */
#sk-container-id-1 label.sk-toggleable__label {
  cursor: pointer;
  display: flex;
  width: 100%;
  margin-bottom: 0;
  padding: 0.5em;
  box-sizing: border-box;
  text-align: center;
  align-items: start;
  justify-content: space-between;
  gap: 0.5em;
}

#sk-container-id-1 label.sk-toggleable__label .caption {
  font-size: 0.6rem;
  font-weight: lighter;
  color: var(--sklearn-color-text-muted);
}

#sk-container-id-1 label.sk-toggleable__label-arrow:before {
  /* Arrow on the left of the label */
  content: "▸";
  float: left;
  margin-right: 0.25em;
  color: var(--sklearn-color-icon);
}

#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {
  color: var(--sklearn-color-text);
}

/* Toggleable content - dropdown */

#sk-container-id-1 div.sk-toggleable__content {
  display: none;
  text-align: left;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content pre {
  margin: 0.2em;
  border-radius: 0.25em;
  color: var(--sklearn-color-text);
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted pre {
  /* unfitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {
  /* Expand drop-down */
  display: block;
  width: 100%;
  overflow: visible;
}

#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {
  content: "▾";
}

/* Pipeline/ColumnTransformer-specific style */

#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator-specific style */

/* Colorize estimator box */
#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

#sk-container-id-1 div.sk-label label.sk-toggleable__label,
#sk-container-id-1 div.sk-label label {
  /* The background is the default theme color */
  color: var(--sklearn-color-text-on-default-background);
}

/* On hover, darken the color of the background */
#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

/* Label box, darken color on hover, fitted */
#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator label */

#sk-container-id-1 div.sk-label label {
  font-family: monospace;
  font-weight: bold;
  display: inline-block;
  line-height: 1.2em;
}

#sk-container-id-1 div.sk-label-container {
  text-align: center;
}

/* Estimator-specific */
#sk-container-id-1 div.sk-estimator {
  font-family: monospace;
  border: 1px dotted var(--sklearn-color-border-box);
  border-radius: 0.25em;
  box-sizing: border-box;
  margin-bottom: 0.5em;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-estimator.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

/* on hover */
#sk-container-id-1 div.sk-estimator:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Specification for estimator info (e.g. "i" and "?") */

/* Common style for "i" and "?" */

.sk-estimator-doc-link,
a:link.sk-estimator-doc-link,
a:visited.sk-estimator-doc-link {
  float: right;
  font-size: smaller;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1em;
  height: 1em;
  width: 1em;
  text-decoration: none !important;
  margin-left: 0.5em;
  text-align: center;
  /* unfitted */
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
  color: var(--sklearn-color-unfitted-level-1);
}

.sk-estimator-doc-link.fitted,
a:link.sk-estimator-doc-link.fitted,
a:visited.sk-estimator-doc-link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
div.sk-estimator:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover,
div.sk-label-container:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover,
div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

/* Span, style for the box shown on hovering the info icon */
.sk-estimator-doc-link span {
  display: none;
  z-index: 9999;
  position: relative;
  font-weight: normal;
  right: .2ex;
  padding: .5ex;
  margin: .5ex;
  width: min-content;
  min-width: 20ex;
  max-width: 50ex;
  color: var(--sklearn-color-text);
  box-shadow: 2pt 2pt 4pt #999;
  /* unfitted */
  background: var(--sklearn-color-unfitted-level-0);
  border: .5pt solid var(--sklearn-color-unfitted-level-3);
}

.sk-estimator-doc-link.fitted span {
  /* fitted */
  background: var(--sklearn-color-fitted-level-0);
  border: var(--sklearn-color-fitted-level-3);
}

.sk-estimator-doc-link:hover span {
  display: block;
}

/* "?"-specific style due to the `<a>` HTML tag */

#sk-container-id-1 a.estimator_doc_link {
  float: right;
  font-size: 1rem;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1rem;
  height: 1rem;
  width: 1rem;
  text-decoration: none;
  /* unfitted */
  color: var(--sklearn-color-unfitted-level-1);
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
}

#sk-container-id-1 a.estimator_doc_link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
#sk-container-id-1 a.estimator_doc_link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

#sk-container-id-1 a.estimator_doc_link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
}

.estimator-table summary {
    padding: .5rem;
    font-family: monospace;
    cursor: pointer;
}

.estimator-table details[open] {
    padding-left: 0.1rem;
    padding-right: 0.1rem;
    padding-bottom: 0.3rem;
}

.estimator-table .parameters-table {
    margin-left: auto !important;
    margin-right: auto !important;
}

.estimator-table .parameters-table tr:nth-child(odd) {
    background-color: #fff;
}

.estimator-table .parameters-table tr:nth-child(even) {
    background-color: #f6f6f6;
}

.estimator-table .parameters-table tr:hover {
    background-color: #e0e0e0;
}

.estimator-table table td {
    border: 1px solid rgba(106, 105, 104, 0.232);
}

.user-set td {
    color:rgb(255, 94, 0);
    text-align: left;
}

.user-set td.value pre {
    color:rgb(255, 94, 0) !important;
    background-color: transparent !important;
}

.default td {
    color: black;
    text-align: left;
}

.user-set td i,
.default td i {
    color: black;
}

.copy-paste-icon {
    background-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCA0NDggNTEyIj48IS0tIUZvbnQgQXdlc29tZSBGcmVlIDYuNy4yIGJ5IEBmb250YXdlc29tZSAtIGh0dHBzOi8vZm9udGF3ZXNvbWUuY29tIExpY2Vuc2UgLSBodHRwczovL2ZvbnRhd2Vzb21lLmNvbS9saWNlbnNlL2ZyZWUgQ29weXJpZ2h0IDIwMjUgRm9udGljb25zLCBJbmMuLS0+PHBhdGggZD0iTTIwOCAwTDMzMi4xIDBjMTIuNyAwIDI0LjkgNS4xIDMzLjkgMTQuMWw2Ny45IDY3LjljOSA5IDE0LjEgMjEuMiAxNC4xIDMzLjlMNDQ4IDMzNmMwIDI2LjUtMjEuNSA0OC00OCA0OGwtMTkyIDBjLTI2LjUgMC00OC0yMS41LTQ4LTQ4bDAtMjg4YzAtMjYuNSAyMS41LTQ4IDQ4LTQ4ek00OCAxMjhsODAgMCAwIDY0LTY0IDAgMCAyNTYgMTkyIDAgMC0zMiA2NCAwIDAgNDhjMCAyNi41LTIxLjUgNDgtNDggNDhMNDggNTEyYy0yNi41IDAtNDgtMjEuNS00OC00OEwwIDE3NmMwLTI2LjUgMjEuNS00OCA0OC00OHoiLz48L3N2Zz4=);
    background-repeat: no-repeat;
    background-size: 14px 14px;
    background-position: 0;
    display: inline-block;
    width: 14px;
    height: 14px;
    cursor: pointer;
}
</style><body><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>BernoulliNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" checked><label for="sk-estimator-id-1" class="sk-toggleable__label fitted sk-toggleable__label-arrow"><div><div>BernoulliNB</div></div><div><a class="sk-estimator-doc-link fitted" rel="noreferrer" target="_blank" href="https://scikit-learn.org/1.7/modules/generated/sklearn.naive_bayes.BernoulliNB.html">?<span>Documentation for BernoulliNB</span></a><span class="sk-estimator-doc-link fitted">i<span>Fitted</span></span></div></label><div class="sk-toggleable__content fitted" data-param-prefix="">
        <div class="estimator-table">
            <details>
                <summary>Parameters</summary>
                <table class="parameters-table">
                  <tbody>
                    
        <tr class="default">
            <td><i class="copy-paste-icon"
                 onclick="copyToClipboard('alpha',
                          this.parentElement.nextElementSibling)"
            ></i></td>
            <td class="param">alpha&nbsp;</td>
            <td class="value">1.0</td>
        </tr>
    

        <tr class="default">
            <td><i class="copy-paste-icon"
                 onclick="copyToClipboard('force_alpha',
                          this.parentElement.nextElementSibling)"
            ></i></td>
            <td class="param">force_alpha&nbsp;</td>
            <td class="value">True</td>
        </tr>
    

        <tr class="default">
            <td><i class="copy-paste-icon"
                 onclick="copyToClipboard('binarize',
                          this.parentElement.nextElementSibling)"
            ></i></td>
            <td class="param">binarize&nbsp;</td>
            <td class="value">0.0</td>
        </tr>
    

        <tr class="default">
            <td><i class="copy-paste-icon"
                 onclick="copyToClipboard('fit_prior',
                          this.parentElement.nextElementSibling)"
            ></i></td>
            <td class="param">fit_prior&nbsp;</td>
            <td class="value">True</td>
        </tr>
    

        <tr class="default">
            <td><i class="copy-paste-icon"
                 onclick="copyToClipboard('class_prior',
                          this.parentElement.nextElementSibling)"
            ></i></td>
            <td class="param">class_prior&nbsp;</td>
            <td class="value">None</td>
        </tr>
    
                  </tbody>
                </table>
            </details>
        </div>
    </div></div></div></div></div><script>function copyToClipboard(text, element) {
    // Get the parameter prefix from the closest toggleable content
    const toggleableContent = element.closest('.sk-toggleable__content');
    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';
    const fullParamName = paramPrefix ? `${paramPrefix}${text}` : text;

    const originalStyle = element.style;
    const computedStyle = window.getComputedStyle(element);
    const originalWidth = computedStyle.width;
    const originalHTML = element.innerHTML.replace('Copied!', '');

    navigator.clipboard.writeText(fullParamName)
        .then(() => {
            element.style.width = originalWidth;
            element.style.color = 'green';
            element.innerHTML = "Copied!";

            setTimeout(() => {
                element.innerHTML = originalHTML;
                element.style = originalStyle;
            }, 2000);
        })
        .catch(err => {
            console.error('Failed to copy:', err);
            element.style.color = 'red';
            element.innerHTML = "Failed!";
            setTimeout(() => {
                element.innerHTML = originalHTML;
                element.style = originalStyle;
            }, 2000);
        });
    return false;
}

document.querySelectorAll('.fa-regular.fa-copy').forEach(function(element) {
    const toggleableContent = element.closest('.sk-toggleable__content');
    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';
    const paramName = element.parentElement.nextElementSibling.textContent.trim();
    const fullParamName = paramPrefix ? `${paramPrefix}${paramName}` : paramName;

    element.setAttribute('title', fullParamName);
});
</script></body></div></div>
</div>
<div class="alert alert-block alert-success"> 
<strong>Deeper Dive:</strong> Interpreting the Bernoulli Naïve Bayes Model Output
  <div style="
    background-color: #efffff;
    color: #000000;
    padding: 10px;
    border-radius: 4px;
    border: 1px solid #dddddd;
    margin-top: 10px;
  ">
<details>
<summary>Explanation</summary>
<p>After fitting the model, Jupyter displays a summary of the BernoulliNB object. This output does not show predictions or results; instead, it reports the configuration parameters that define how the model learned from the data and how it will behave during prediction. These parameters control <em>how probabilities are estimated</em> and <em>how the learned decision rule is constructed</em>.</p>
<hr class="docutils" />
<p><strong>Model type: <code class="docutils literal notranslate"><span class="pre">BernoulliNB</span></code></strong></p>
<p>This confirms that the classifier is designed for <strong>binary features</strong> (0 or 1), such as molecular fingerprint bits. Each feature is treated as a Bernoulli random variable indicating presence or absence.</p>
<hr class="docutils" />
<p><strong><code class="docutils literal notranslate"><span class="pre">alpha</span> <span class="pre">=</span> <span class="pre">1.0</span></code> — smoothing parameter</strong></p>
<p>This parameter controls Laplace smoothing, which prevents estimated probabilities from becoming exactly 0 or 1.</p>
<ul class="simple">
<li><p>If a fingerprint bit never appears within a given class—that is, for all training samples with a particular class label ( y ), the feature value ( <span class="math notranslate nohighlight">\(x_i = 0\)</span> ), then without smoothing the model would assign a probability of zero to that feature for that class. During prediction, this would cause the entire class to be ruled out whenever that bit is present. Laplace smoothing instead assigns a small, nonzero probability.</p></li>
<li><p>This avoids numerical issues and makes the model more robust, especially when working with limited or sparse data.</p></li>
<li><p>This is different from the earlier feature selection step, where we removed features that did not change across the entire dataset (for example, the MACCS bit position that was always zero for all molecules).</p></li>
</ul>
<p><strong>Smoothing ensures that no single missing feature completely rules out a class.</strong></p>
<hr class="docutils" />
<p><strong><code class="docutils literal notranslate"><span class="pre">force_alpha</span> <span class="pre">=</span> <span class="pre">True</span></code></strong></p>
<p>This forces the model to always use the specified value of <code class="docutils literal notranslate"><span class="pre">alpha</span></code>.</p>
<ul class="simple">
<li><p>Ensures consistent smoothing behavior</p></li>
<li><p>Mainly an internal safety setting</p></li>
</ul>
<p>For most users, this parameter does not need to be changed.</p>
<hr class="docutils" />
<p><strong><code class="docutils literal notranslate"><span class="pre">binarize</span> <span class="pre">=</span> <span class="pre">0.0</span></code></strong></p>
<p>This parameter controls whether input features are automatically converted to binary values.</p>
<ul class="simple">
<li><p>Values <strong>greater than 0.0 → 1</strong></p></li>
<li><p>Values <strong>less than or equal to 0.0 → 0</strong></p></li>
</ul>
<p>In this workflow:</p>
<ul class="simple">
<li><p>Molecular fingerprints are already binary</p></li>
<li><p>This parameter effectively has no impact</p></li>
</ul>
<hr class="docutils" />
<p><strong><code class="docutils literal notranslate"><span class="pre">fit_prior</span> <span class="pre">=</span> <span class="pre">True</span></code></strong></p>
<p>This tells the model to learn class priors from the training labels (<code class="docutils literal notranslate"><span class="pre">y_train</span></code>).</p>
<ul class="simple">
<li><p>If the training data are balanced, the learned priors will be equal</p></li>
<li><p>If the training data are imbalanced, the learned priors will reflect that imbalance, giving more weight to the more common class.</p></li>
</ul>
<p>As a result, when fit_prior=True, class frequency influences how strongly the model favors one class over another. In an imbalanced dataset, the model will require stronger feature evidence to predict the minority class. Setting <code class="docutils literal notranslate"><span class="pre">fit_prior=False</span></code> forces the model to treat all classes as equally likely, which may be useful in some cases but can lead to unreliable probability estimates when the training data are highly imbalanced, and so the default is true.</p>
<hr class="docutils" />
<p><strong><code class="docutils literal notranslate"><span class="pre">class_prior</span> <span class="pre">=</span> <span class="pre">None</span></code></strong></p>
<p>This indicates that no manual class priors were provided.</p>
<ul class="simple">
<li><p>Because <code class="docutils literal notranslate"><span class="pre">fit_prior</span> <span class="pre">=</span> <span class="pre">True</span></code> and <code class="docutils literal notranslate"><span class="pre">class_prior</span> <span class="pre">=</span> <span class="pre">None</span></code>, the model estimates class priors automatically from the training labels (<code class="docutils literal notranslate"><span class="pre">y_train</span></code>).</p></li>
<li><p>In other words, the frequency of each class in the training data determines how strongly the model expects each class before considering any features.</p></li>
</ul>
<p>It is also possible to override this behavior by supplying class priors explicitly when the model is created (for example, to force equal weighting of classes). However, in this lesson we allow the model to infer priors from the training data so that the connection between data preparation (such as downsampling) and model behavior remains transparent.</p>
<hr class="docutils" />
<p><strong>Big-picture takeaway</strong></p>
<p>Together, these parameters define <em>how</em> Bernoulli Naïve Bayes:</p>
<ul class="simple">
<li><p>Estimates feature probabilities within each class</p></li>
<li><p>Incorporates class balance through priors</p></li>
<li><p>Constructs a decision rule that maps feature vectors <strong>X</strong> to class labels <strong>y</strong></p></li>
</ul>
<p>The model has now learned this decision rule internally. In the next step, we will apply it to new data to see how well it generalizes beyond the training set.</p>
</details>
</div>
</div>
</section>
<section id="from-training-to-inference-what-does-the-classifier-produce">
<h2>3.2 From Training to Inference: What does the Classifier Produce?<a class="headerlink" href="#from-training-to-inference-what-does-the-classifier-produce" title="Link to this heading">#</a></h2>
<p>Model evaluation depends on what type of output the classifier produces: class labels or probabilities.</p>
<p>Once a classifier has been trained, it can produce two different kinds of outputs, depending on which method is used. Understanding this distinction is essential, because different evaluation metrics require different types of outputs.</p>
<p>The method <code class="docutils literal notranslate"><span class="pre">.predict()</span></code> returns discrete class labels. For a binary classifier, these are integers such as <code class="docutils literal notranslate"><span class="pre">0</span></code> (inactive) or <code class="docutils literal notranslate"><span class="pre">1</span></code> (active). Calling <code class="docutils literal notranslate"><span class="pre">.predict()</span></code> answers the question: <em>“Which class does the model assign to each compound?”</em> These class labels are used to construct confusion matrices and to compute metrics such as accuracy, precision, recall, and F1-score.</p>
<p>In contrast, <code class="docutils literal notranslate"><span class="pre">.predict_proba()</span></code> returns probabilistic scores. For each compound, the model reports its estimated probability of belonging to each class. For binary classification, this means a pair of values that sum to 1. These probabilities answer a different question: <em>“How confident is the model in its prediction?”</em> Probabilistic outputs are required for threshold-independent metrics such as ROC curves and ROC–AUC.</p>
<p>The difference can be seen directly by running the following code and we will go over these in the next two sections</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Discrete class predictions (0 or 1)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">clf_NB</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># Probabilistic predictions (confidence scores)</span>
<span class="n">y_proba</span> <span class="o">=</span> <span class="n">clf_NB</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="n">y_pred</span><span class="p">[:</span><span class="mi">5</span><span class="p">],</span> <span class="n">y_proba</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(array([0, 0, 1, 0, 0]),
 array([[9.99990160e-01, 9.83971842e-06],
        [5.73893051e-01, 4.26106949e-01],
        [6.58510978e-03, 9.93414890e-01],
        [9.48421570e-01, 5.15784297e-02],
        [8.86150183e-01, 1.13849817e-01]]))
</pre></div>
</div>
</div>
</div>
<div class="alert alert-block alert-info">
<strong>Deeper Dive:</strong> From <em>y = f(X)</em> to predictions in scikit-learn
  <div style="
    background-color: #efffff;
    color: #000000;
    padding: 10px;
    border-radius: 4px;
    border: 1px solid #dddddd;
    margin-top: 10px;
  ">
<details>
<summary>Explanation</summary>
<p>
The <code>scikit-learn</code> library follows the <code>X</code>/<code>y</code>
conventions consistently when fitting models and generating predictions.
</p>
<ul>
  <li>Models are trained using <code>model.fit(X, y)</code></li>
  <li><code>X</code> is treated as a feature matrix</li>
  <li><code>y</code> is treated as a target vector</li>
</ul>
<p>
For <b>probabilistic classifiers</b> such as Naive Bayes, prediction occurs in
two conceptual steps:
</p>
<p style="text-align: center;">
\( \mathbf{X} \;\rightarrow\; P(y \mid \mathbf{X}) \;\rightarrow\; \hat{y} \)
</p>
<ul>
  <li>
    <b><code>X</code></b> — the feature matrix
    <ul>
      <li>Numerical representation of the compounds</li>
      <li>Each row = one compound</li>
      <li>Each column = one descriptor or fingerprint bit</li>
    </ul>
  </li>
  <li>
    <b>\( P(y \mid X) \)</b> — the <b>posterior probability</b>
    <ul>
      <li>The model’s estimated probability of each class</li>
      <li>Computed using Bayes’ theorem</li>
      <li>Returned by <code>model.predict_proba(X)</code></li>
    </ul>
  </li>
  <li>
    <b>\( \hat{y} \)</b> — the <b>predicted class label</b>
    <ul>
      <li>The single class chosen for each compound</li>
      <li>Selected as the class with the highest probability</li>
      <li>Returned by <code>model.predict(X)</code></li>
    </ul>
  </li>
</ul>
<p>
Here, <code>predict_proba()</code> computes probabilities, while
<code>predict()</code> applies a decision rule based on those probabilities.
</p>
<p>
Even when Pandas DataFrames or Series are used, scikit-learn converts them
internally to <b>NumPy arrays</b>, preserving the matrix–vector structure
implied by <code>X</code> and <code>y</code>.
</p>
</details>
</div>
</div>
<div class="alert alert-block alert-warning">
In the sections that follow we will focus on what does the model output, and how do we compute standard evaluation quantities? In <strong><a href="../../../appendices/A-10-3_ConfusionMatrix.ipynb">Module 10_3_Model Evaluation</a> </strong> we will explore these concepts in depth and what they tell us about models, the data, and the scientific task.
</div></section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="classification-based-inference-predict">
<h1>4. Classification-Based Inference (<code class="docutils literal notranslate"><span class="pre">.predict</span></code>)<a class="headerlink" href="#classification-based-inference-predict" title="Link to this heading">#</a></h1>
<p>With a trained classifier in hand, we now turn to its use for classification-based inference. In this mode, the model assigns each compound to a discrete class, such as inactive (0) or active (1), based on the learned decision rule. Evaluating the model at this stage means examining how these class assignments compare to known reference labels and analyzing the types of classification errors that occur.</p>
<p>In scikit-learn, classification evaluation is performed by comparing two aligned arrays:</p>
<ul class="simple">
<li><p>the known class labels for a dataset, often referred to conceptually as “ground truth” (<code class="docutils literal notranslate"><span class="pre">y_train_bal</span></code> or <code class="docutils literal notranslate"><span class="pre">y_test</span></code>)</p></li>
<li><p>the class labels predicted by the model using <code class="docutils literal notranslate"><span class="pre">.predict()</span></code> (<code class="docutils literal notranslate"><span class="pre">y_pred</span></code>)</p></li>
</ul>
<p>Rather than introducing a separate variable such as <code class="docutils literal notranslate"><span class="pre">y_true</span></code>, this notebook consistently uses the existing dataset labels (<code class="docutils literal notranslate"><span class="pre">y_train_bal</span></code> or <code class="docutils literal notranslate"><span class="pre">y_test</span></code>) to make it explicit which dataset is being evaluated and to reinforce the distinction between training and test data. This section focuses on classification outcomes and begins with the confusion matrix, which provides a complete summary of how predicted class labels compare to known labels. Summary metrics derived from the confusion matrix are introduced afterward.</p>
<section id="confusion-matrix-for-classification-evaluation">
<h2>4.1 Confusion Matrix for Classification Evaluation<a class="headerlink" href="#confusion-matrix-for-classification-evaluation" title="Link to this heading">#</a></h2>
<p>To evaluate how well the trained Bernoulli Naïve Bayes model classifies compounds, we compare the model’s predicted class labels to the known reference labels using a confusion matrix. A confusion matrix always compares two one-dimensional arrays:</p>
<ul class="simple">
<li><p>the reference labels for a dataset (<code class="docutils literal notranslate"><span class="pre">y_train_bal</span></code> or <code class="docutils literal notranslate"><span class="pre">y_test</span></code>)</p></li>
<li><p>the predicted class labels produced by the model for the same samples (<code class="docutils literal notranslate"><span class="pre">y_pred</span></code>)</p></li>
</ul>
<p>In this section, evaluation is performed using the test set, which provides an unbiased estimate of classification performance on unseen data. The known labels are therefore <code class="docutils literal notranslate"><span class="pre">y_test</span></code>, and the predicted labels (<code class="docutils literal notranslate"><span class="pre">y_pred</span></code>) are generated by applying the trained model to <code class="docutils literal notranslate"><span class="pre">X_test</span></code>.</p>
<p>The resulting confusion matrix organizes predictions into four categories: true positives, true negatives, false positives, and false negatives. These counts form the foundation for many commonly used classification metrics and provide direct insight into the types of errors the model makes.</p>
</section>
<section id="load-feature-arrays-and-generate-the-confusion-matrix">
<h2>4.2 Load Feature Arrays and Generate the Confusion Matrix<a class="headerlink" href="#load-feature-arrays-and-generate-the-confusion-matrix" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Imports and paths</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">pathlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">Path</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="n">SPLIT_ROOT</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">&quot;data/AID743139/splits/90_10/arrays&quot;</span><span class="p">)</span>

<span class="c1"># Load saved feature arrays and labels</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">SPLIT_ROOT</span> <span class="o">/</span> <span class="s2">&quot;X_train.npy&quot;</span><span class="p">)</span>
<span class="n">X_test</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">SPLIT_ROOT</span> <span class="o">/</span> <span class="s2">&quot;X_test.npy&quot;</span><span class="p">)</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">SPLIT_ROOT</span> <span class="o">/</span> <span class="s2">&quot;y_train.npy&quot;</span><span class="p">)</span>
<span class="n">y_test</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">SPLIT_ROOT</span> <span class="o">/</span> <span class="s2">&quot;y_test.npy&quot;</span><span class="p">)</span>

<span class="c1"># Balance the training set (downsample inactives)</span>
<span class="n">idx_inactives</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">y_train</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">idx_actives</span>   <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">y_train</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">idx_inactives_down</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span>
    <span class="n">idx_inactives</span><span class="p">,</span>
    <span class="n">size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">idx_actives</span><span class="p">),</span>
    <span class="n">replace</span><span class="o">=</span><span class="kc">False</span>
<span class="p">)</span>

<span class="n">X_train_bal</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span>
    <span class="n">X_train</span><span class="p">[</span><span class="n">idx_inactives_down</span><span class="p">],</span>
    <span class="n">X_train</span><span class="p">[</span><span class="n">idx_actives</span><span class="p">]</span>
<span class="p">))</span>

<span class="n">y_train_bal</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span>
    <span class="n">y_train</span><span class="p">[</span><span class="n">idx_inactives_down</span><span class="p">],</span>
    <span class="n">y_train</span><span class="p">[</span><span class="n">idx_actives</span><span class="p">]</span>
<span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Train the classifier</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.naive_bayes</span><span class="w"> </span><span class="kn">import</span> <span class="n">BernoulliNB</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">classification_report</span>

<span class="n">clf_NB</span> <span class="o">=</span> <span class="n">BernoulliNB</span><span class="p">()</span>
<span class="n">clf_NB</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_bal</span><span class="p">,</span> <span class="n">y_train_bal</span><span class="p">)</span>

<span class="c1"># Generate test-set predictions</span>
<span class="n">y_test_pred</span> <span class="o">=</span> <span class="n">clf_NB</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># Confusion matrix</span>
<span class="n">CMat</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_test_pred</span><span class="p">)</span>
<span class="n">TN</span><span class="p">,</span> <span class="n">FP</span><span class="p">,</span> <span class="n">FN</span><span class="p">,</span> <span class="n">TP</span> <span class="o">=</span> <span class="n">CMat</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test-set Confusion Matrix&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">CMat</span><span class="p">)</span>

<span class="c1"># Classification report</span>
<span class="c1">#|print(&quot;\nTest-set Classification Report&quot;)</span>
<span class="c1">#print(classification_report(y_test, y_test_pred))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Test-set Confusion Matrix
[[400 206]
 [ 25  49]]
</pre></div>
</div>
</div>
</div>
<p>These two lines perform the core evaluation step.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">clf_NB</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">CMat</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">CMat</span><span class="p">)</span>    <span class="c1"># [[TN, FP], </span>
               <span class="c1">#  [FN, TP]]</span>
<span class="c1"># Extracting TN, FP, FN, TP from the confusion matrix               </span>
<span class="n">TN</span> <span class="o">=</span> <span class="n">CMat</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>  <span class="c1"># True Negatives</span>
<span class="n">FP</span> <span class="o">=</span> <span class="n">CMat</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>  <span class="c1"># False Positives</span>
<span class="n">FN</span> <span class="o">=</span> <span class="n">CMat</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>  <span class="c1"># False Negatives</span>
<span class="n">TP</span> <span class="o">=</span> <span class="n">CMat</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>  <span class="c1"># True Positives</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;True Negatives (TN):&quot;</span><span class="p">,</span> <span class="n">TN</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;False Positives (FP):&quot;</span><span class="p">,</span> <span class="n">FP</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;False Negatives (FN):&quot;</span><span class="p">,</span> <span class="n">FN</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;True Positives (TP):&quot;</span><span class="p">,</span> <span class="n">TP</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Total predictions:&quot;</span><span class="p">,</span> <span class="n">TN</span> <span class="o">+</span> <span class="n">FP</span> <span class="o">+</span> <span class="n">FN</span> <span class="o">+</span> <span class="n">TP</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[400 206]
 [ 25  49]]
True Negatives (TN): 400
False Positives (FP): 206
False Negatives (FN): 25
True Positives (TP): 49
Total predictions: 680
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># -------------------------------------------------</span>
<span class="c1"># Result persistence setup</span>
<span class="c1"># -------------------------------------------------</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">pathlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">Path</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">json</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">datetime</span><span class="w"> </span><span class="kn">import</span> <span class="n">date</span>

<span class="n">RESULTS_ROOT</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">&quot;../results/AID743139/nb&quot;</span><span class="p">)</span>
<span class="n">RESULTS_ROOT</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">parents</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Results will be saved to:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">RESULTS_ROOT</span><span class="o">.</span><span class="n">resolve</span><span class="p">())</span>

<span class="c1"># -------------------------------------------------</span>
<span class="c1"># Save test-set predictions</span>
<span class="c1"># -------------------------------------------------</span>
<span class="c1"># y_test       : known labels (ground truth)</span>
<span class="c1"># y_test_pred  : model predictions</span>

<span class="n">df_test_pred</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
    <span class="s2">&quot;y_true&quot;</span><span class="p">:</span> <span class="n">y_test</span><span class="p">,</span> <span class="c1"># y_true : reference labels for evaluation (here: y_test)</span>
    <span class="s2">&quot;y_pred&quot;</span><span class="p">:</span> <span class="n">y_test_pred</span> <span class="c1"># y_pred : labels predicted by the model</span>
<span class="p">})</span>

<span class="n">pred_path</span> <span class="o">=</span> <span class="n">RESULTS_ROOT</span> <span class="o">/</span> <span class="s2">&quot;test_predictions.csv&quot;</span>
<span class="n">df_test_pred</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="n">pred_path</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Saved test-set predictions to </span><span class="si">{</span><span class="n">pred_path</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">df_test_pred</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Results will be saved to:
/home/rebelford/jupyterbooks/cinf26book/content/modules/results/AID743139/nb
Saved test-set predictions to test_predictions.csv
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>y_true</th>
      <th>y_pred</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
<section id="saving-confusion-matrices-and-metrics">
<h2>4.3 Saving Confusion Matrices and Metrics<a class="headerlink" href="#saving-confusion-matrices-and-metrics" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># -------------------------------------------------</span>
<span class="c1"># Save confusion matrix</span>
<span class="c1"># -------------------------------------------------</span>
<span class="n">cm_path</span> <span class="o">=</span> <span class="n">RESULTS_ROOT</span> <span class="o">/</span> <span class="s2">&quot;confusion_matrix_test.npy&quot;</span>
<span class="n">np</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">cm_path</span><span class="p">,</span> <span class="n">CMat</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Saved confusion matrix to </span><span class="si">{</span><span class="n">cm_path</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Confusion matrix contents:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">CMat</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Saved confusion matrix to confusion_matrix_test.npy
Confusion matrix contents:
[[400 206]
 [ 25  49]]
</pre></div>
</div>
</div>
</div>
<section id="recording-experimental-metadata">
<h3>4.3.1 Recording Experimental Metadata<a class="headerlink" href="#recording-experimental-metadata" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># -------------------------------------------------</span>
<span class="c1"># Save evaluation metrics and metadata</span>
<span class="c1"># -------------------------------------------------</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">accuracy_score</span>

<span class="n">metrics</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;accuracy&quot;</span><span class="p">:</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_test_pred</span><span class="p">),</span>
    <span class="s2">&quot;TN&quot;</span><span class="p">:</span> <span class="nb">int</span><span class="p">(</span><span class="n">TN</span><span class="p">),</span>
    <span class="s2">&quot;FP&quot;</span><span class="p">:</span> <span class="nb">int</span><span class="p">(</span><span class="n">FP</span><span class="p">),</span>
    <span class="s2">&quot;FN&quot;</span><span class="p">:</span> <span class="nb">int</span><span class="p">(</span><span class="n">FN</span><span class="p">),</span>
    <span class="s2">&quot;TP&quot;</span><span class="p">:</span> <span class="nb">int</span><span class="p">(</span><span class="n">TP</span><span class="p">)</span>
<span class="p">}</span>

<span class="n">metrics_path</span> <span class="o">=</span> <span class="n">RESULTS_ROOT</span> <span class="o">/</span> <span class="s2">&quot;metrics_test.json&quot;</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">metrics_path</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">json</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">metrics</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">metadata</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;assay&quot;</span><span class="p">:</span> <span class="s2">&quot;AID743139&quot;</span><span class="p">,</span>
    <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="s2">&quot;BernoulliNB&quot;</span><span class="p">,</span>
    <span class="s2">&quot;features&quot;</span><span class="p">:</span> <span class="s2">&quot;MACCS&quot;</span><span class="p">,</span>
    <span class="s2">&quot;split&quot;</span><span class="p">:</span> <span class="s2">&quot;90_10&quot;</span><span class="p">,</span>
    <span class="s2">&quot;training_balanced&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
    <span class="s2">&quot;evaluation_set&quot;</span><span class="p">:</span> <span class="s2">&quot;test&quot;</span><span class="p">,</span>
    <span class="s2">&quot;date&quot;</span><span class="p">:</span> <span class="n">date</span><span class="o">.</span><span class="n">today</span><span class="p">()</span><span class="o">.</span><span class="n">isoformat</span><span class="p">(),</span>
    <span class="s2">&quot;notebook&quot;</span><span class="p">:</span> <span class="s2">&quot;10.2_naive_bayes.ipynb&quot;</span>
<span class="p">}</span>

<span class="n">metadata_path</span> <span class="o">=</span> <span class="n">RESULTS_ROOT</span> <span class="o">/</span> <span class="s2">&quot;results_metadata.json&quot;</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">metadata_path</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">json</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">metadata</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Saved metrics and metadata:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot; - </span><span class="si">{</span><span class="n">metrics_path</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot; - </span><span class="si">{</span><span class="n">metadata_path</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Saved metrics and metadata:
 - metrics_test.json
 - results_metadata.json
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="interpreting-the-confusion-matrix">
<h2>4.4 Interpreting the Confusion Matrix<a class="headerlink" href="#interpreting-the-confusion-matrix" title="Link to this heading">#</a></h2>
<p>There is no single number that fully describes how a classifier performs. Instead of immediately collapsing model behavior into a summary statistic, we begin by examining the confusion matrix, which records how predictions are distributed across correct and incorrect outcomes. By separating true positives, false positives, true negatives, and false negatives, the confusion matrix makes explicit which types of mistakes the model makes and how often they occur. This structured view provides the foundation for all subsequent evaluation metrics.</p>
<blockquote>
<div><p><em>Historically, the confusion matrix is so named because it explicitly shows where a model “confuses” one class for another, rather than hiding those errors inside a single summary value.</em> Simply speaking, a False positive was when the model thought  an inactive compound was active, and a false negative is when the model predicts an active compound is inactive.</p>
</div></blockquote>
<p>In the sections that follow, we will use the confusion matrix to derive quantitative evaluation measures. Each metric is computed directly from these counts, and their meaning is best understood only after the structure of the confusion matrix is clear.</p>
<p>In this class we will use the scikit-learn convention above, where labels are sorted <span class="math notranslate nohighlight">\([0,1]\)</span> and align with binary encoding 0 (inactive) and 1 (active).</p>
<div style="font-family: Arial, sans-serif; margin-top: 20px;">
<center>
  Confusion Matrix (scikit-learn Convention)
  <table border="1" cellspacing="0" cellpadding="10" style="border-collapse: collapse; text-align: center;">
    <tr>
      <th rowspan="2">Actual</th>
      <th colspan="2">Predicted</th>
    </tr>
    <tr>
      <th>0</th>
      <th>1</th>
    </tr>
    <tr>
      <th>0</th>
      <td>TN<br><small>True Negative</small></td>
      <td>FP<br><small>False Positive</small></td>
    </tr>
    <tr>
      <th>1</th>
      <td>FN<br><small>False Negative</small></td>
      <td>TP<br><small>True Positive</small></td>
    </tr>
  </table>
</center>
</div>
<p>You need to be aware that there is an alternate convention that starts with the positive <span class="math notranslate nohighlight">\([1,0]\)</span> type ordering.</p>
<div class="alert alert-block alert-success"> 
<strong>Alternate Convention:</strong> Important: There is no single universal visual convention for confusion matrices. Different fields and textbooks place the “positive” and “negative” classes in different positions. What matters is being explicit and consistent. In this notebook, we follow the scikit-learn convention used by confusion_matrix(y_true, y_pred): 
  <div style="
    background-color: #efffff;
    color: #000000;
    padding: 10px;
    border-radius: 4px;
    border: 1px solid #dddddd;
    margin-top: 10px;
  ">
<details>
<summary>Explanation</summary>
Many sources choose to place the positive first and negative second, essentially reversing the boolean relationships, as shown in the following confusion matrix.
<div style="font-family: Arial, sans-serif; margin-top: 20px;">
<center>
  Confusion Matrix (Positive-First / Conceptual Convention)
  <table border="1" cellspacing="0" cellpadding="10" style="border-collapse: collapse; text-align: center;">
    <tr>
      <th rowspan="2">Actual</th>
      <th colspan="2">Predicted</th>
    </tr>
    <tr>
      <th>1</th>
      <th>0</th>
    </tr>
    <tr>
      <th>1</th>
      <td>TP<br><small>True Positive</small></td>
      <td>FN<br><small>False Negative</small></td>
    </tr>
    <tr>
      <th>0</th>
      <td>FP<br><small>False Positive</small></td>
      <td>TN<br><small>True Negative</small></td>
    </tr>
  </table>
</center>
</div>
<p>We will not use this convention in this class, but when comparing data you need to be aware of which convention is being used.</p>
</details>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span>
    <span class="n">CMat</span><span class="p">,</span>
    <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">fmt</span><span class="o">=</span><span class="s2">&quot;d&quot;</span><span class="p">,</span>
    <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;Greens&quot;</span><span class="p">,</span>
    <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">xticklabels</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Predicted 0 (Inactive)&quot;</span><span class="p">,</span> <span class="s2">&quot;Predicted 1 (Active)&quot;</span><span class="p">],</span>
    <span class="n">yticklabels</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Actual 0 (Inactive)&quot;</span><span class="p">,</span> <span class="s2">&quot;Actual 1 (Active)&quot;</span><span class="p">],</span>
    <span class="n">linewidths</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">linecolor</span><span class="o">=</span><span class="s2">&quot;black&quot;</span>
<span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Confusion Matrix — Bernoulli Naïve Bayes&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Predicted Label&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Actual Label&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/262854df656d5250fac257bed54ab40fe73a5f61d8aad8616c8f2ba07e79e013.png" src="../../../_images/262854df656d5250fac257bed54ab40fe73a5f61d8aad8616c8f2ba07e79e013.png" />
</div>
</div>
<p>The diagonal cells represent correct predictions, while the off-diagonal cells represent errors. The confusion matrix makes it visually clear that different types of errors occur, which is why we need multiple evaluation metrics rather than a single accuracy value</p>
</section>
<section id="confusion-matrix-based-evaluation-metrics">
<h2>4.5 Confusion Matrix Based Evaluation Metrics<a class="headerlink" href="#confusion-matrix-based-evaluation-metrics" title="Link to this heading">#</a></h2>
<p>The confusion matrix reveals which kinds of mistakes the model makes.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p><strong>Predicted Inactive</strong></p></th>
<th class="head"><p><strong>Predicted Active</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Actual Inactive</strong></p></td>
<td><p><strong>TN</strong> (True Negative)</p></td>
<td><p><strong>FP</strong> (False Positive)</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Actual Active</strong></p></td>
<td><p><strong>FN</strong> (False Negative)</p></td>
<td><p><strong>TP</strong> (True Positive)</p></td>
</tr>
</tbody>
</table>
</div>
<section id="table-of-confusion-matrix-metrics">
<h3>4.5.1 Table of Confusion Matrix Metrics<a class="headerlink" href="#table-of-confusion-matrix-metrics" title="Link to this heading">#</a></h3>
<p>All the metrics below are derived from TN, TP, FN &amp; FP and summarize the model’s validity (mistakes) from different scientific perspectives.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Metric</p></th>
<th class="head"><p>Mathematical Definition</p></th>
<th class="head"><p>What This Metric Tells Us</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Accuracy</strong></p></td>
<td><p><span class="math notranslate nohighlight">\(\displaystyle \frac{TP + TN}{TP + TN + FP + FN}\)</span></p></td>
<td><p>What fraction of all predictions—active and inactive—were correct overall.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Precision</strong></p></td>
<td><p><span class="math notranslate nohighlight">\(\displaystyle \frac{TP}{TP + FP}\)</span></p></td>
<td><p>When the model predicts “active,” how often is it actually correct?</p></td>
</tr>
<tr class="row-even"><td><p><strong>Sensitivity (Recall)</strong></p></td>
<td><p><span class="math notranslate nohighlight">\(\displaystyle \frac{TP}{TP + FN}\)</span></p></td>
<td><p>Of all truly active compounds, how many did the model successfully identify?</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Specificity</strong></p></td>
<td><p><span class="math notranslate nohighlight">\(\displaystyle \frac{TN}{TN + FP}\)</span></p></td>
<td><p>Of all truly inactive compounds, how many did the model correctly reject?</p></td>
</tr>
<tr class="row-even"><td><p><strong>Balanced Accuracy</strong></p></td>
<td><p><span class="math notranslate nohighlight">\(\displaystyle \frac{\text{Sensitivity} + \text{Specificity}}{2}\)</span></p></td>
<td><p>How well the model performs across both classes, even when the dataset is imbalanced.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>F1 Score</strong></p></td>
<td><p><span class="math notranslate nohighlight">\(\displaystyle \frac{2,(\text{Precision}\times\text{Sensitivity})}{\text{Precision} + \text{Sensitivity}}\)</span></p></td>
<td><p>A single score that balances finding actives with avoiding false positives.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># --- Test metrics ---</span>
<span class="n">acc_test</span>  <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_test_pred</span><span class="p">)</span>
<span class="n">prec_test</span> <span class="o">=</span> <span class="n">TP</span> <span class="o">/</span> <span class="p">(</span><span class="n">TP</span> <span class="o">+</span> <span class="n">FP</span><span class="p">)</span> <span class="k">if</span> <span class="p">(</span><span class="n">TP</span> <span class="o">+</span> <span class="n">FP</span><span class="p">)</span> <span class="k">else</span> <span class="mf">0.0</span>
<span class="n">sens_test</span> <span class="o">=</span> <span class="n">TP</span> <span class="o">/</span> <span class="p">(</span><span class="n">TP</span> <span class="o">+</span> <span class="n">FN</span><span class="p">)</span> <span class="k">if</span> <span class="p">(</span><span class="n">TP</span> <span class="o">+</span> <span class="n">FN</span><span class="p">)</span> <span class="k">else</span> <span class="mf">0.0</span>
<span class="n">spec_test</span> <span class="o">=</span> <span class="n">TN</span> <span class="o">/</span> <span class="p">(</span><span class="n">TN</span> <span class="o">+</span> <span class="n">FP</span><span class="p">)</span> <span class="k">if</span> <span class="p">(</span><span class="n">TN</span> <span class="o">+</span> <span class="n">FP</span><span class="p">)</span> <span class="k">else</span> <span class="mf">0.0</span>
<span class="n">bacc_test</span> <span class="o">=</span> <span class="p">(</span><span class="n">sens_test</span> <span class="o">+</span> <span class="n">spec_test</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
<span class="n">f1_test</span>   <span class="o">=</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">prec_test</span> <span class="o">*</span> <span class="n">sens_test</span> <span class="o">/</span> <span class="p">(</span><span class="n">prec_test</span> <span class="o">+</span> <span class="n">sens_test</span><span class="p">))</span> <span class="k">if</span> <span class="p">(</span><span class="n">prec_test</span> <span class="o">+</span> <span class="n">sens_test</span><span class="p">)</span> <span class="k">else</span> <span class="mf">0.0</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;TEST SET PERFORMANCE&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Accuracy          = </span><span class="si">{</span><span class="n">acc_test</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Precision         = </span><span class="si">{</span><span class="n">prec_test</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sensitivity       = </span><span class="si">{</span><span class="n">sens_test</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Specificity       = </span><span class="si">{</span><span class="n">spec_test</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Balanced Accuracy = </span><span class="si">{</span><span class="n">bacc_test</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;F1 Score          = </span><span class="si">{</span><span class="n">f1_test</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>TEST SET PERFORMANCE
Accuracy          = 0.6603
Precision         = 0.1922
Sensitivity       = 0.6622
Specificity       = 0.6601
Balanced Accuracy = 0.6611
F1 Score          = 0.2979
</pre></div>
</div>
</div>
</div>
<p>The key takeaway from this discussion is that confusion-matrix-based metrics describe model performance at a single decision threshold, and some of those metrics, especially precision and F1, are highly sensitive to class imbalance. As a result, a model may contain useful discriminatory information while still appearing unreliable at a particular cutoff. Before deciding whether a model is truly useful, we therefore need a way to evaluate its ability to separate actives from inactives independently of any single threshold. This motivates a ranking-based evaluation using the ROC–AUC curve, which assesses whether the model assigns higher scores to active compounds than to inactive ones across all possible decision thresholds.</p>
<div class="alert alert-block alert-warning">
Evaluation of the Confusion Matrix will continue in<strong><a href="../../../appendices/A-10-3_ConfusionMatrix.ipynb"> Module 10_3_Model Evaluation</a> </strong>, but before we do this we need to introduce probability-based inference and ROC-AUC
</div></section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="probability-based-inference-predict-proba">
<h1>5. Probability-Based Inference (<code class="docutils literal notranslate"><span class="pre">.predict_proba</span></code>)<a class="headerlink" href="#probability-based-inference-predict-proba" title="Link to this heading">#</a></h1>
<p>Up to this point, we have evaluated our classifier using a confusion matrix and metrics derived from it (accuracy, sensitivity, specificity, etc.). All of those metrics are based on a single decision rule: the model assigns each compound to a class (active or inactive) using a fixed threshold. This raises an important question:</p>
<blockquote>
<div><p><em>What if the model is capable of separating actives from inactives, but the particular threshold we chose is not ideal?</em></p>
</div></blockquote>
<p>The key limitation of a confusion matrix is that it evaluates the model at only one operating point—that is, at a single decision threshold. If that threshold is poorly chosen, the model may appear to perform badly even when it has learned meaningful structure in the data.</p>
<p>The ROC curve (Receiver Operating Characteristic curve) addresses this limitation by evaluating the model across all possible decision thresholds. At one extreme, when the threshold is set to 1, no compounds are predicted active; at the other extreme, when the threshold is set to 0, all compounds are predicted active. As the threshold is gradually lowered between these extremes, the ROC curve traces how the true positive rate (sensitivity) increases in relation to the false positive rate, revealing the trade-off between correctly identifying actives and incorrectly flagging inactives.</p>
<p>Historically, the term “receiver operating characteristic” comes from signal-detection theory, where it described how a signal receiver’s performance changed as its detection sensitivity was adjusted. In machine learning, the same idea applies: the ROC curve characterizes how a classifier’s behavior changes as we vary the decision rule that converts scores into classifications.</p>
<p>The AUC (Area Under the Curve) then condenses this threshold-sweep into a single number, representing the integrated area under the ROC curve. Conceptually, AUC measures how well the model can rank compounds independent of any particular cutoff: if active compounds tend to receive higher scores than inactive ones, the AUC will be high, even if no single threshold has yet been chosen.</p>
<p>In this way, ROC–AUC answers a different question than the confusion matrix:</p>
<ul class="simple">
<li><p>Confusion matrices ask: <em>“How did the model perform at this particular decision rule?”</em></p></li>
<li><p>ROC–AUC asks: <em>“Across all possible decision rules, does the model separate actives from inactives at all?”</em></p></li>
</ul>
<p>This distinction is especially important in imbalanced datasets, where accuracy and related metrics can be misleading. A model may appear weak at a single threshold while still containing useful discriminatory information. ROC–AUC allows us to detect whether that underlying signal exists before we commit to a specific classification cutoff.</p>
<p>In the next steps, we will compute ROC–AUC using the same trained model as before. The only change will be how we interpret its output: instead of hard class labels, we will use the model’s continuous scores. Once we understand how well the model separates the classes in general, we can then return to the question of choosing an appropriate threshold for a specific scientific or practical objective.</p>
<div class="alert alert-block alert-info">
<strong>Reality Check: What Do We Mean by “Threshold”?</strong>  The word <em>threshold</em> appears in both experimental science and machine learning, but it refers to two very different ideas. Confusing these meanings can make ROC–AUC feel mysterious or even misleading.
<p><strong>Discussion question:</strong><br>
When we talk about a “threshold” in ROC analysis, are we talking about the same kind of threshold that defines whether a compound is active or inactive in a bioassay?</p>
<div class="alert alert-block alert-success">
<details>
  <summary>Explanation</summary>
  <p><strong>No — these are fundamentally different thresholds.</strong></p>
  <p><strong>Experimental (bioassay) threshold:</strong><br>
  In an assay, a threshold is part of how the <em>labels are defined</em>. For example, a compound might be called “active” if its IC<sub>50</sub> is below a certain concentration. This threshold is tied to biology, chemistry, and experimental design. Once chosen, it defines the ground truth labels used for modeling.</p>
  <p><strong>Model (decision) threshold:</strong><br>
  In machine learning, the threshold does <em>not</em> change the experimental labels. Instead, it controls how the model’s <em>continuous output</em> (such as a predicted probability of being active) is converted into a hard class prediction (active vs inactive).</p>
  <p>For example, a model might assign a compound a score of 0.62 for being active. If we choose a decision threshold of 0.50, the model predicts “active.” If we choose a threshold of 0.70, the same compound would be predicted “inactive.” The underlying experimental label has not changed — only the model’s decision rule has.</p>
  <p><strong>Why this matters for ROC–AUC:</strong><br>
ROC–AUC is fundamentally a ranking-based evaluation. Rather than measuring performance at a single cutoff, it evaluates whether the model’s scores induce a ranked ordering in which truly active compounds tend to receive higher scores than truly inactive ones. The higher the AUC, the greater the probability that a randomly chosen active compound outranks a randomly chosen inactive compound.
<p>As the decision threshold is varied from 1 to 0, we are effectively sliding a cutoff down this ranked list. Each possible threshold corresponds to a different point on the ROC curve. If the ranking meaningfully separates actives from inactives, then there will exist one or more thresholds that produce useful classification performance.</p>
<p>The AUC summarizes this idea by measuring how consistently the model ranks active compounds ahead of inactive ones, independent of where the cutoff is ultimately placed. If AUC is close to 0.5, the ranking is essentially random and no threshold will meaningfully separate the classes. If AUC is high, the ranking contains real discriminatory information, and the remaining task becomes choosing an appropriate threshold to turn that ranking into decisions.  ROC analysis explores <em>all possible decision thresholds</em> applied to the model’s scores. It asks whether the model tends to assign higher scores to truly active compounds than to truly inactive ones, regardless of where we ultimately place the cutoff.</p></p>
</details>
</div>
</details><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. Imports</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">pathlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">Path</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.naive_bayes</span><span class="w"> </span><span class="kn">import</span> <span class="n">BernoulliNB</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">confusion_matrix</span><span class="p">,</span>
    <span class="n">accuracy_score</span><span class="p">,</span>
    <span class="n">roc_auc_score</span><span class="p">,</span>
    <span class="n">roc_curve</span>
<span class="p">)</span>

<span class="c1">#2 Load arrays (uncomment if SPLIT_ROOT is not in memory)</span>
<span class="c1">#SPLIT_ROOT = Path(&quot;data/AID743139/splits/90_10/arrays&quot;)</span>

<span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">SPLIT_ROOT</span> <span class="o">/</span> <span class="s2">&quot;X_train.npy&quot;</span><span class="p">)</span>
<span class="n">X_test</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">SPLIT_ROOT</span> <span class="o">/</span> <span class="s2">&quot;X_test.npy&quot;</span><span class="p">)</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">SPLIT_ROOT</span> <span class="o">/</span> <span class="s2">&quot;y_train.npy&quot;</span><span class="p">)</span>
<span class="n">y_test</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">SPLIT_ROOT</span> <span class="o">/</span> <span class="s2">&quot;y_test.npy&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y_test</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># 3. Balance the training set</span>
<span class="n">idx_inactives</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">y_train</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">idx_actives</span>   <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">y_train</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">num_actives</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">idx_actives</span><span class="p">)</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">idx_inactives_downsampled</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span>
    <span class="n">idx_inactives</span><span class="p">,</span>
    <span class="n">size</span><span class="o">=</span><span class="n">num_actives</span><span class="p">,</span>
    <span class="n">replace</span><span class="o">=</span><span class="kc">False</span>
<span class="p">)</span>

<span class="n">X_train_bal</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span>
    <span class="n">X_train</span><span class="p">[</span><span class="n">idx_inactives_downsampled</span><span class="p">],</span>
    <span class="n">X_train</span><span class="p">[</span><span class="n">idx_actives</span><span class="p">]</span>
<span class="p">))</span>

<span class="n">y_train_bal</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span>
    <span class="n">y_train</span><span class="p">[</span><span class="n">idx_inactives_downsampled</span><span class="p">],</span>
    <span class="n">y_train</span><span class="p">[</span><span class="n">idx_actives</span><span class="p">]</span>
<span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="n">X_train_bal</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y_train_bal</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(6113, 162) (6113,)
(680, 162) (680,)
(1338, 162) (1338,)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 4 Fit model</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">BernoulliNB</span><span class="p">()</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_bal</span><span class="p">,</span> <span class="n">y_train_bal</span><span class="p">)</span>

<span class="c1">#5 Predict Hard Labels</span>
<span class="n">y_pred_test</span>  <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">y_pred_train</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train_bal</span><span class="p">)</span>

<span class="c1"># 6. Create confusion matrices (train and test)</span>

<span class="n">CMat_test</span>  <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_test</span><span class="p">)</span>
<span class="n">CMat_train</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_train_bal</span><span class="p">,</span> <span class="n">y_pred_train</span><span class="p">)</span>

<span class="n">TN</span><span class="p">,</span> <span class="n">FP</span><span class="p">,</span> <span class="n">FN</span><span class="p">,</span> <span class="n">TP</span> <span class="o">=</span> <span class="n">CMat_test</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
<span class="n">TN_tr</span><span class="p">,</span> <span class="n">FP_tr</span><span class="p">,</span> <span class="n">FN_tr</span><span class="p">,</span> <span class="n">TP_tr</span> <span class="o">=</span> <span class="n">CMat_train</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test confusion matrix:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">CMat_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Training confusion matrix:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">CMat_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Test confusion matrix:
 [[400 206]
 [ 25  49]]

Training confusion matrix:
 [[457 212]
 [187 482]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 7. Metrics from confusion matrices</span>
<span class="c1"># --- Test metrics ---</span>
<span class="n">acc_test</span>  <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_test</span><span class="p">)</span>
<span class="n">prec_test</span> <span class="o">=</span> <span class="n">TP</span> <span class="o">/</span> <span class="p">(</span><span class="n">TP</span> <span class="o">+</span> <span class="n">FP</span><span class="p">)</span> <span class="k">if</span> <span class="p">(</span><span class="n">TP</span> <span class="o">+</span> <span class="n">FP</span><span class="p">)</span> <span class="k">else</span> <span class="mf">0.0</span>
<span class="n">sens_test</span> <span class="o">=</span> <span class="n">TP</span> <span class="o">/</span> <span class="p">(</span><span class="n">TP</span> <span class="o">+</span> <span class="n">FN</span><span class="p">)</span> <span class="k">if</span> <span class="p">(</span><span class="n">TP</span> <span class="o">+</span> <span class="n">FN</span><span class="p">)</span> <span class="k">else</span> <span class="mf">0.0</span>
<span class="n">spec_test</span> <span class="o">=</span> <span class="n">TN</span> <span class="o">/</span> <span class="p">(</span><span class="n">TN</span> <span class="o">+</span> <span class="n">FP</span><span class="p">)</span> <span class="k">if</span> <span class="p">(</span><span class="n">TN</span> <span class="o">+</span> <span class="n">FP</span><span class="p">)</span> <span class="k">else</span> <span class="mf">0.0</span>
<span class="n">bacc_test</span> <span class="o">=</span> <span class="p">(</span><span class="n">sens_test</span> <span class="o">+</span> <span class="n">spec_test</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
<span class="n">f1_test</span>   <span class="o">=</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">prec_test</span> <span class="o">*</span> <span class="n">sens_test</span> <span class="o">/</span> <span class="p">(</span><span class="n">prec_test</span> <span class="o">+</span> <span class="n">sens_test</span><span class="p">))</span> <span class="k">if</span> <span class="p">(</span><span class="n">prec_test</span> <span class="o">+</span> <span class="n">sens_test</span><span class="p">)</span> <span class="k">else</span> <span class="mf">0.0</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;TEST SET PERFORMANCE&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Accuracy          = </span><span class="si">{</span><span class="n">acc_test</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Precision         = </span><span class="si">{</span><span class="n">prec_test</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sensitivity       = </span><span class="si">{</span><span class="n">sens_test</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Specificity       = </span><span class="si">{</span><span class="n">spec_test</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Balanced Accuracy = </span><span class="si">{</span><span class="n">bacc_test</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;F1 Score          = </span><span class="si">{</span><span class="n">f1_test</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># --- Training metrics ---</span>
<span class="n">acc_train</span>  <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_train_bal</span><span class="p">,</span> <span class="n">y_pred_train</span><span class="p">)</span>
<span class="n">prec_train</span> <span class="o">=</span> <span class="n">TP_tr</span> <span class="o">/</span> <span class="p">(</span><span class="n">TP_tr</span> <span class="o">+</span> <span class="n">FP_tr</span><span class="p">)</span> <span class="k">if</span> <span class="p">(</span><span class="n">TP_tr</span> <span class="o">+</span> <span class="n">FP_tr</span><span class="p">)</span> <span class="k">else</span> <span class="mf">0.0</span>
<span class="n">sens_train</span> <span class="o">=</span> <span class="n">TP_tr</span> <span class="o">/</span> <span class="p">(</span><span class="n">TP_tr</span> <span class="o">+</span> <span class="n">FN_tr</span><span class="p">)</span> <span class="k">if</span> <span class="p">(</span><span class="n">TP_tr</span> <span class="o">+</span> <span class="n">FN_tr</span><span class="p">)</span> <span class="k">else</span> <span class="mf">0.0</span>
<span class="n">spec_train</span> <span class="o">=</span> <span class="n">TN_tr</span> <span class="o">/</span> <span class="p">(</span><span class="n">TN_tr</span> <span class="o">+</span> <span class="n">FP_tr</span><span class="p">)</span> <span class="k">if</span> <span class="p">(</span><span class="n">TN_tr</span> <span class="o">+</span> <span class="n">FP_tr</span><span class="p">)</span> <span class="k">else</span> <span class="mf">0.0</span>
<span class="n">bacc_train</span> <span class="o">=</span> <span class="p">(</span><span class="n">sens_train</span> <span class="o">+</span> <span class="n">spec_train</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
<span class="n">f1_train</span>   <span class="o">=</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">prec_train</span> <span class="o">*</span> <span class="n">sens_train</span> <span class="o">/</span> <span class="p">(</span><span class="n">prec_train</span> <span class="o">+</span> <span class="n">sens_train</span><span class="p">))</span> <span class="k">if</span> <span class="p">(</span><span class="n">prec_train</span> <span class="o">+</span> <span class="n">sens_train</span><span class="p">)</span> <span class="k">else</span> <span class="mf">0.0</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">TRAINING SET PERFORMANCE&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Accuracy          = </span><span class="si">{</span><span class="n">acc_train</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Precision         = </span><span class="si">{</span><span class="n">prec_train</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sensitivity       = </span><span class="si">{</span><span class="n">sens_train</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Specificity       = </span><span class="si">{</span><span class="n">spec_train</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Balanced Accuracy = </span><span class="si">{</span><span class="n">bacc_train</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;F1 Score          = </span><span class="si">{</span><span class="n">f1_train</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>TEST SET PERFORMANCE
Accuracy          = 0.6603
Precision         = 0.1922
Sensitivity       = 0.6622
Specificity       = 0.6601
Balanced Accuracy = 0.6611
F1 Score          = 0.2979

TRAINING SET PERFORMANCE
Accuracy          = 0.7018
Precision         = 0.6945
Sensitivity       = 0.7205
Specificity       = 0.6831
Balanced Accuracy = 0.7018
F1 Score          = 0.7073
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#8. Get probablilty scores</span>
<span class="n">y_score_test</span>  <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">y_score_train</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_train_bal</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span>

<span class="c1">#9. Compute ROC-AUC (train and test)</span>
<span class="n">auc_test</span>  <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_score_test</span><span class="p">)</span>
<span class="n">auc_train</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_train_bal</span><span class="p">,</span> <span class="n">y_score_train</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;ROC–AUC&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Test AUC     = </span><span class="si">{</span><span class="n">auc_test</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Training AUC = </span><span class="si">{</span><span class="n">auc_train</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>ROC–AUC
Test AUC     = 0.7279
Training AUC = 0.7546
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fpr_test</span><span class="p">,</span> <span class="n">tpr_test</span><span class="p">,</span> <span class="n">thresholds_test</span> <span class="o">=</span> <span class="n">roc_curve</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_score_test</span><span class="p">)</span>
<span class="n">fpr_train</span><span class="p">,</span> <span class="n">tpr_train</span><span class="p">,</span> <span class="n">thresholds_train</span> <span class="o">=</span> <span class="n">roc_curve</span><span class="p">(</span><span class="n">y_train_bal</span><span class="p">,</span> <span class="n">y_score_train</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">&quot;ROC curve arrays (test set):</span><span class="se">\n</span><span class="s2">&quot;</span>
    <span class="sa">f</span><span class="s2">&quot;  fpr_test        = </span><span class="si">{</span><span class="n">fpr_test</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">  # False Positive Rate values</span><span class="se">\n</span><span class="s2">&quot;</span>
    <span class="sa">f</span><span class="s2">&quot;  tpr_test        = </span><span class="si">{</span><span class="n">tpr_test</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">  # True Positive Rate values</span><span class="se">\n</span><span class="s2">&quot;</span>
    <span class="sa">f</span><span class="s2">&quot;  thresholds_test = </span><span class="si">{</span><span class="n">thresholds_test</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">  # Decision thresholds</span><span class="se">\n</span><span class="s2">&quot;</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">&quot;ROC curve arrays (test set):</span><span class="se">\n</span><span class="s2">&quot;</span>
    <span class="sa">f</span><span class="s2">&quot;  fpr_train        = </span><span class="si">{</span><span class="n">fpr_train</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">  # False Positive Rate values</span><span class="se">\n</span><span class="s2">&quot;</span>
    <span class="sa">f</span><span class="s2">&quot;  tpr_train        = </span><span class="si">{</span><span class="n">tpr_train</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">  # True Positive Rate values</span><span class="se">\n</span><span class="s2">&quot;</span>
    <span class="sa">f</span><span class="s2">&quot;  thresholds_train = </span><span class="si">{</span><span class="n">thresholds_train</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">  # Decision thresholds</span><span class="se">\n</span><span class="s2">&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>ROC curve arrays (test set):
  fpr_test        = (144,)  # False Positive Rate values
  tpr_test        = (144,)  # True Positive Rate values
  thresholds_test = (144,)  # Decision thresholds

ROC curve arrays (test set):
  fpr_train        = (587,)  # False Positive Rate values
  tpr_train        = (587,)  # True Positive Rate values
  thresholds_train = (587,)  # Decision thresholds
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="c1"># Plot ROC curves</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">fpr_test</span><span class="p">,</span>  <span class="n">tpr_test</span><span class="p">,</span>  <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Test ROC (AUC = </span><span class="si">{</span><span class="n">auc_test</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">fpr_train</span><span class="p">,</span> <span class="n">tpr_train</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Train ROC (AUC = </span><span class="si">{</span><span class="n">auc_train</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

<span class="c1"># Reference line: random classifier</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Random (AUC = 0.5)&quot;</span><span class="p">)</span>

<span class="c1"># Labels and title</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;False Positive Rate (1 − Specificity)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;True Positive Rate (Sensitivity)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;ROC Curve: BernoulliNB with MACCS Fingerprints&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;lower right&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/1aa7536f8fab88fccbc2b9ee6914f1dcc92a3489020761baea5605dcc4205344.png" src="../../../_images/1aa7536f8fab88fccbc2b9ee6914f1dcc92a3489020761baea5605dcc4205344.png" />
</div>
</div>
<p>The ROC curves for both the training and test sets lie well above the diagonal reference line, indicating that the model has learned a meaningful ranking signal rather than behaving like a random classifier. The training curve consistently exceeds the test curve, as expected, but the gap between them is modest, suggesting limited overfitting and reasonable generalization. The test-set AUC of approximately 0.72 confirms that active compounds tend to receive higher scores than inactive compounds, even though no single decision threshold has yet been chosen. At the same time, the gradual rise of the curve highlights an inherent trade-off: increasing sensitivity necessarily increases the false positive rate. This reinforces the idea that ROC–AUC evaluates whether useful separation exists at all, while the choice of a specific threshold must be guided by scientific or practical priorities.</p>
<div class="alert alert-block alert-success">
<H1>Homework</H1>
<strong>Problem 1: hw_10.2my_AID.ipynb</strong> 
<p>Make a new notebook called hw_10.2my_AID.ipynb and place it in the directory you have been runnin this notebook in.  You now need to create a confusion matric, and ROC and print out AOC values. I do not want you to generate all of the narrative of this notebook, just the code cells with maybe headers and a few markdown statements to assist someone in navigating the notebook. There are several approaches you can take, for example, you could make a copy of this notebook, rename it, delete the bulk of the markdown cells, and then use refactoring to change the AID used in this notebook to your AID.  Or you may want to open the Jupyter book in another cell, and just copy those cells you need for the workflow.  You need to use the paths techniques and cinf26pk, so I can load your completed notebook and generate the data artifacts within the directory architecture we are developing in this class
</p>
</div></section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "datachem"
        },
        kernelOptions: {
            name: "datachem",
            path: "./content/modules/10_SupervisedML"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'datachem'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="10_1_data_prep.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">10.1: Data Preparation and Feature Engineering</p>
      </div>
    </a>
    <a class="right-next"
       href="../../appendices/App_1/README.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Appendix 1.1: Getting Set Up</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Module 10.2: Naive Bayes and Model Construction</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#preparation-for-model-building">1. Preparation for model building</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#loading-the-data-into-x-and-y">1.1 Loading the data into X and y.</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-selection-remove-zero-variance-features">1.2 Feature Selection: Remove zero-variance features</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-selection-freezing-the-feature-definition-mask-and-metadata">1.3 Feature Selection: Freezing the Feature Definition (Mask and Metadata)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reload-data">1.4 Reload Data</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regenerate-x-maccs">1.4.1 Regenerate X_MACCS</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#load-and-apply-saved-variance-mask">1.4.2 Load and apply saved variance mask</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reconstruct-the-filtered-feature-matrix-x-maccs-filtered">1.4.3 Reconstruct the filtered feature matrix <code class="docutils literal notranslate"><span class="pre">X_MACCS_filtered</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#train-test-split-a-9-1-ratio">1.5 Train-Test-Split (a 9:1 ratio)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#save-the-test-train-split-as-np-arrays">1.5.1 Save the test/train split as np arrays</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#preparing-the-training-data-model-construction">2. Preparing the Training Data Model Construction</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reload-the-saved-training-and-test-arrays">2.1 Reload the Saved Training and Test Arrays</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#examine-class-imbalance">2.2 Examine class imbalance</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-class-imbalance-and-why-it-matters">2.2.1 What is class imbalance and why it matters</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#measuring-class-imbalance-in-the-training-set">2.2.2 Measuring Class Imbalance in the Training Set</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#balance-the-training-set-by-downsampling">2.3 Balance the Training Set by Downsampling</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">2.3 Balance the training set by downsampling</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#build-a-model-using-the-training-set">3. Build a model using the training set.</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#naive-bayes">3.1 Naïve Bayes</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#from-training-to-inference-what-does-the-classifier-produce">3.2 From Training to Inference: What does the Classifier Produce?</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#classification-based-inference-predict">4. Classification-Based Inference (<code class="docutils literal notranslate"><span class="pre">.predict</span></code>)</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#confusion-matrix-for-classification-evaluation">4.1 Confusion Matrix for Classification Evaluation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#load-feature-arrays-and-generate-the-confusion-matrix">4.2 Load Feature Arrays and Generate the Confusion Matrix</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#saving-confusion-matrices-and-metrics">4.3 Saving Confusion Matrices and Metrics</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recording-experimental-metadata">4.3.1 Recording Experimental Metadata</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interpreting-the-confusion-matrix">4.4 Interpreting the Confusion Matrix</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#confusion-matrix-based-evaluation-metrics">4.5 Confusion Matrix Based Evaluation Metrics</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#table-of-confusion-matrix-metrics">4.5.1 Table of Confusion Matrix Metrics</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-based-inference-predict-proba">5. Probability-Based Inference (<code class="docutils literal notranslate"><span class="pre">.predict_proba</span></code>)</a></li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Robert Belford
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright CC 4.0 2026 Robert Belford, additional info on each notebook.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  <script async src="https://hypothes.is/embed.js"></script>

</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>